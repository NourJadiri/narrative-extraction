{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def load_annotations_df(path):\n",
    "    annotations_df = pd.read_csv(path, sep='\\t', header=None)\n",
    "\n",
    "    annotations_df.columns = ['id', 'narratives', 'subnarratives']\n",
    "\n",
    "    annotations_df['narratives'] = annotations_df['narratives'].apply(lambda x: x.split(';'))\n",
    "    annotations_df['subnarratives'] = annotations_df['subnarratives'].apply(lambda x: x.split(';'))\n",
    "\n",
    "    return annotations_df\n",
    "\n",
    "annotations_df = load_annotations_df('data/EN/subtask-2-annotations.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to extract the list of all unique subnarratives\n",
    "def get_subnarratives_list(file):\n",
    "    \"\"\"\n",
    "    Extracts subnarratives from the nested JSON structure.\n",
    "    \n",
    "    Args:\n",
    "        data (dict): The JSON-like dictionary containing narratives.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of subnarratives with the hierarchy preserved in their names.\n",
    "    \"\"\"\n",
    "    subnarratives = [\"Other\"]\n",
    "    with open(file, 'r') as f:    \n",
    "        data = json.load(f)\n",
    "        for main_category, subcategories in data.items():\n",
    "            for subcategory, narratives in subcategories.items():\n",
    "                if \"Other\" not in narratives:\n",
    "                    narratives.append(\"Other\")\n",
    "\n",
    "                for narrative in narratives:\n",
    "                    subnarratives.append(f\"{main_category}: {subcategory}: {narrative}\")\n",
    "        \n",
    "    return subnarratives\n",
    "\n",
    "def get_narratives_list(file):\n",
    "    \"\"\"\n",
    "    Extracts narratives from the nested JSON structure.\n",
    "    \n",
    "    Args:\n",
    "        data (dict): The JSON-like dictionary containing narratives.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of narratives with the hierarchy preserved in their names.\n",
    "    \"\"\"\n",
    "    narratives = [\"Other\"]\n",
    "    with open(file, 'r') as f:    \n",
    "        data = json.load(f)\n",
    "        for main_category, subcategories in data.items():\n",
    "            for subcategory, narrative in subcategories.items():\n",
    "                narratives.append(f\"{main_category}: {subcategory}\")\n",
    "        \n",
    "    return narratives\n",
    "\n",
    "narratives_list = get_narratives_list('data/taxonomy.json')\n",
    "subnarratives_list = get_subnarratives_list('data/taxonomy.json')\n",
    "taxonomy = json.load(open('data/taxonomy.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def read_text(file_id, base_path='data/EN/raw-documents'):\n",
    "    with open(os.path.join(base_path, f'{file_id}'), 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "    \n",
    "def get_sibling_subnarratives(subnarrative):\n",
    "    \"\"\"\n",
    "    Get all the sibling subnarratives of the given subnarrative.\n",
    "    \n",
    "    Args:\n",
    "        subnarrative (str): The subnarrative for which to find siblings.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of sibling subnarratives.\n",
    "    \"\"\"\n",
    "    return taxonomy[subnarrative.split(': ')[0]][subnarrative.split(': ')[1]]\n",
    "\n",
    "def get_narrative_definition(narrative):\n",
    "    if narrative == 'Other':\n",
    "        return 'Statements that are NOT related to anyone of these topics : {}'.format(', '.join([get_narrative_short_name(narrative) for narrative in narratives_list if narrative != 'Other']))\n",
    "    narrative_definitions = pd.read_csv('data/narratives definition.csv')\n",
    "    short_name = narrative.split(':')[-1].strip()\n",
    "    return narrative_definitions[narrative_definitions['narrative'] == short_name]['definition'].values[0]\n",
    "\n",
    "def get_narrative_examples(narrative):\n",
    "    if narrative == 'Other':\n",
    "        return None\n",
    "    narrative_definitions = pd.read_csv('data/narratives definition.csv')\n",
    "    short_name = narrative.split(':')[-1].strip()\n",
    "    return narrative_definitions[narrative_definitions['narrative'] == short_name]['example'].values[0]\n",
    "\n",
    "def get_narrative_short_name(narrative):\n",
    "    return narrative.split(':')[-1].strip()\n",
    "\n",
    "def get_subnarrative_definition(subnarrative):\n",
    "    if subnarrative == 'Other':\n",
    "        return 'Statements that are NOT related to anyone of these narratives : {}'.format(', '.join([narrative for narrative in narratives_list if narrative != 'Other']))\n",
    "    short_name = subnarrative.split(':')[-1].strip()\n",
    "    narrative = subnarrative.split(':')[-2].strip()\n",
    "    narrative_defintion = get_narrative_definition(narrative)\n",
    "    if short_name == 'Other':\n",
    "        return 'Statement that are related to the narrative \"{}\", defined as {} but are not related to anyone of these subnarratives : {}'.format(get_narrative_short_name(narrative), narrative_defintion ,get_sibling_subnarratives(subnarrative))\n",
    "    subnarrative_definitions = pd.read_csv('data/subnarrative definitions.csv')\n",
    "    return subnarrative_definitions[subnarrative_definitions['subnarrative'] == short_name]['definition'].values[0]\n",
    "\n",
    "\n",
    "def get_subnarrative_examples(subnarrative):\n",
    "    if subnarrative == 'Other':\n",
    "        return None\n",
    "    short_name = subnarrative.split(':')[-1].strip()\n",
    "    if short_name == 'Other':\n",
    "        return None\n",
    "    subnarrative_definitions = pd.read_csv('data/subnarrative definitions.csv')\n",
    "    return subnarrative_definitions[subnarrative_definitions['subnarrative'] == short_name]['examples'].values[0]\n",
    "\n",
    "def get_subnarrative_short_name(subnarrative):\n",
    "    return subnarrative.split(':')[-1].strip()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to create dataframe for subnarrative: Other\n",
      "Attempting to create dataframe for subnarrative: URW: Blaming the war on others rather than the invader: Ukraine is the aggressor\n",
      "Attempting to create dataframe for subnarrative: URW: Blaming the war on others rather than the invader: The West are the aggressors\n",
      "Attempting to create dataframe for subnarrative: URW: Blaming the war on others rather than the invader: Other\n",
      "Attempting to create dataframe for subnarrative: URW: Discrediting Ukraine: Rewriting Ukraineâ€™s history\n",
      "Attempting to create dataframe for subnarrative: URW: Discrediting Ukraine: Discrediting Ukrainian nation and society\n",
      "Attempting to create dataframe for subnarrative: URW: Discrediting Ukraine: Discrediting Ukrainian military\n",
      "Attempting to create dataframe for subnarrative: URW: Discrediting Ukraine: Discrediting Ukrainian government and officials and policies\n",
      "Attempting to create dataframe for subnarrative: URW: Discrediting Ukraine: Ukraine is a puppet of the West\n",
      "Attempting to create dataframe for subnarrative: URW: Discrediting Ukraine: Ukraine is a hub for criminal activities\n",
      "Attempting to create dataframe for subnarrative: URW: Discrediting Ukraine: Ukraine is associated with nazism\n",
      "Attempting to create dataframe for subnarrative: URW: Discrediting Ukraine: Situation in Ukraine is hopeless\n",
      "Attempting to create dataframe for subnarrative: URW: Discrediting Ukraine: Other\n",
      "Attempting to create dataframe for subnarrative: URW: Russia is the Victim: The West is russophobic\n",
      "Attempting to create dataframe for subnarrative: URW: Russia is the Victim: Russia actions in Ukraine are only self-defence\n",
      "Attempting to create dataframe for subnarrative: URW: Russia is the Victim: UA is anti-RU extremists\n",
      "Attempting to create dataframe for subnarrative: URW: Russia is the Victim: Other\n",
      "Attempting to create dataframe for subnarrative: URW: Praise of Russia: Praise of Russian military might\n",
      "Attempting to create dataframe for subnarrative: URW: Praise of Russia: Praise of Russian President Vladimir Putin\n",
      "Attempting to create dataframe for subnarrative: URW: Praise of Russia: Russia is a guarantor of peace and prosperity\n",
      "Attempting to create dataframe for subnarrative: URW: Praise of Russia: Russia has international support from a number of countries and people\n",
      "Attempting to create dataframe for subnarrative: URW: Praise of Russia: Russian invasion has strong national support\n",
      "Attempting to create dataframe for subnarrative: URW: Praise of Russia: Other\n",
      "Attempting to create dataframe for subnarrative: URW: Overpraising the West: NATO will destroy Russia\n",
      "Attempting to create dataframe for subnarrative: URW: Overpraising the West: The West belongs in the right side of history\n",
      "Attempting to create dataframe for subnarrative: URW: Overpraising the West: The West has the strongest international support\n",
      "Attempting to create dataframe for subnarrative: URW: Overpraising the West: Other\n",
      "Attempting to create dataframe for subnarrative: URW: Speculating war outcomes: Russian army is collapsing\n",
      "Attempting to create dataframe for subnarrative: URW: Speculating war outcomes: Russian army will lose all the occupied territories\n",
      "Attempting to create dataframe for subnarrative: URW: Speculating war outcomes: Ukrainian army is collapsing\n",
      "Attempting to create dataframe for subnarrative: URW: Speculating war outcomes: Other\n",
      "Attempting to create dataframe for subnarrative: URW: Discrediting the West, Diplomacy: The EU is divided\n",
      "Attempting to create dataframe for subnarrative: URW: Discrediting the West, Diplomacy: The West is weak\n",
      "Attempting to create dataframe for subnarrative: URW: Discrediting the West, Diplomacy: The West is overreacting\n",
      "Attempting to create dataframe for subnarrative: URW: Discrediting the West, Diplomacy: The West does not care about Ukraine, only about its interests\n",
      "Attempting to create dataframe for subnarrative: URW: Discrediting the West, Diplomacy: Diplomacy does/will not work\n",
      "Attempting to create dataframe for subnarrative: URW: Discrediting the West, Diplomacy: West is tired of Ukraine\n",
      "Attempting to create dataframe for subnarrative: URW: Discrediting the West, Diplomacy: Other\n",
      "Attempting to create dataframe for subnarrative: URW: Negative Consequences for the West: Sanctions imposed by Western countries will backfire\n",
      "Attempting to create dataframe for subnarrative: URW: Negative Consequences for the West: The conflict will increase the Ukrainian refugee flows to Europe\n",
      "Attempting to create dataframe for subnarrative: URW: Negative Consequences for the West: Other\n",
      "Attempting to create dataframe for subnarrative: URW: Distrust towards Media: Western media is an instrument of propaganda\n",
      "Attempting to create dataframe for subnarrative: URW: Distrust towards Media: Ukrainian media cannot be trusted\n",
      "Attempting to create dataframe for subnarrative: URW: Distrust towards Media: Other\n",
      "Attempting to create dataframe for subnarrative: URW: Amplifying war-related fears: By continuing the war we risk WWIII\n",
      "Attempting to create dataframe for subnarrative: URW: Amplifying war-related fears: Russia will also attack other countries\n",
      "Attempting to create dataframe for subnarrative: URW: Amplifying war-related fears: There is a real possibility that nuclear weapons will be employed\n",
      "Attempting to create dataframe for subnarrative: URW: Amplifying war-related fears: NATO should/will directly intervene\n",
      "Attempting to create dataframe for subnarrative: URW: Amplifying war-related fears: Other\n",
      "Attempting to create dataframe for subnarrative: URW: Hidden plots by secret schemes of powerful groups: Other\n",
      "Attempting to create dataframe for subnarrative: CC: Criticism of climate policies: Climate policies are ineffective\n",
      "Attempting to create dataframe for subnarrative: CC: Criticism of climate policies: Climate policies have negative impact on the economy\n",
      "Attempting to create dataframe for subnarrative: CC: Criticism of climate policies: Climate policies are only for profit\n",
      "Attempting to create dataframe for subnarrative: CC: Criticism of climate policies: Other\n",
      "Attempting to create dataframe for subnarrative: CC: Criticism of institutions and authorities: Criticism of the EU\n",
      "Attempting to create dataframe for subnarrative: CC: Criticism of institutions and authorities: Criticism of international entities\n",
      "Attempting to create dataframe for subnarrative: CC: Criticism of institutions and authorities: Criticism of national governments\n",
      "Attempting to create dataframe for subnarrative: CC: Criticism of institutions and authorities: Criticism of political organizations and figures\n",
      "Attempting to create dataframe for subnarrative: CC: Criticism of institutions and authorities: Other\n",
      "Attempting to create dataframe for subnarrative: CC: Climate change is beneficial: CO2 is beneficial\n",
      "Attempting to create dataframe for subnarrative: CC: Climate change is beneficial: Temperature increase is beneficial\n",
      "Attempting to create dataframe for subnarrative: CC: Climate change is beneficial: Other\n",
      "Attempting to create dataframe for subnarrative: CC: Downplaying climate change: Climate cycles are natural\n",
      "Attempting to create dataframe for subnarrative: CC: Downplaying climate change: Weather suggests the trend is global cooling\n",
      "Attempting to create dataframe for subnarrative: CC: Downplaying climate change: Temperature increase does not have significant impact\n",
      "Attempting to create dataframe for subnarrative: CC: Downplaying climate change: CO2 concentrations are too small to have an impact\n",
      "Attempting to create dataframe for subnarrative: CC: Downplaying climate change: Human activities do not impact climate change\n",
      "Attempting to create dataframe for subnarrative: CC: Downplaying climate change: Ice is not melting\n",
      "Attempting to create dataframe for subnarrative: CC: Downplaying climate change: Sea levels are not rising\n",
      "Attempting to create dataframe for subnarrative: CC: Downplaying climate change: Humans and nature will adapt to the changes\n",
      "Attempting to create dataframe for subnarrative: CC: Downplaying climate change: Other\n",
      "Attempting to create dataframe for subnarrative: CC: Questioning the measurements and science: Methodologies/metrics used are unreliable/faulty\n",
      "Attempting to create dataframe for subnarrative: CC: Questioning the measurements and science: Data shows no temperature increase\n",
      "Attempting to create dataframe for subnarrative: CC: Questioning the measurements and science: Greenhouse effect/carbon dioxide do not drive climate change\n",
      "Attempting to create dataframe for subnarrative: CC: Questioning the measurements and science: Scientific community is unreliable\n",
      "Attempting to create dataframe for subnarrative: CC: Questioning the measurements and science: Other\n",
      "Attempting to create dataframe for subnarrative: CC: Criticism of climate movement: Climate movement is alarmist\n",
      "Attempting to create dataframe for subnarrative: CC: Criticism of climate movement: Climate movement is corrupt\n",
      "Attempting to create dataframe for subnarrative: CC: Criticism of climate movement: Ad hominem attacks on key activists\n",
      "Attempting to create dataframe for subnarrative: CC: Criticism of climate movement: Other\n",
      "Attempting to create dataframe for subnarrative: CC: Controversy about green technologies: Renewable energy is dangerous\n",
      "Attempting to create dataframe for subnarrative: CC: Controversy about green technologies: Renewable energy is unreliable\n",
      "Attempting to create dataframe for subnarrative: CC: Controversy about green technologies: Renewable energy is costly\n",
      "Attempting to create dataframe for subnarrative: CC: Controversy about green technologies: Nuclear energy is not climate friendly\n",
      "Attempting to create dataframe for subnarrative: CC: Controversy about green technologies: Other\n",
      "Attempting to create dataframe for subnarrative: CC: Hidden plots by secret schemes of powerful groups: Blaming global elites\n",
      "Attempting to create dataframe for subnarrative: CC: Hidden plots by secret schemes of powerful groups: Climate agenda has hidden motives\n",
      "Attempting to create dataframe for subnarrative: CC: Hidden plots by secret schemes of powerful groups: Other\n",
      "Attempting to create dataframe for subnarrative: CC: Amplifying Climate Fears: Earth will be uninhabitable soon\n",
      "Attempting to create dataframe for subnarrative: CC: Amplifying Climate Fears: Amplifying existing fears of global warming\n",
      "Attempting to create dataframe for subnarrative: CC: Amplifying Climate Fears: Doomsday scenarios for humans\n",
      "Attempting to create dataframe for subnarrative: CC: Amplifying Climate Fears: Whatever we do it is already too late\n",
      "Attempting to create dataframe for subnarrative: CC: Amplifying Climate Fears: Other\n",
      "Attempting to create dataframe for subnarrative: CC: Green policies are geopolitical instruments: Climate-related international relations are abusive/exploitative\n",
      "Attempting to create dataframe for subnarrative: CC: Green policies are geopolitical instruments: Green activities are a form of neo-colonialism\n",
      "Attempting to create dataframe for subnarrative: CC: Green policies are geopolitical instruments: Other\n"
     ]
    }
   ],
   "source": [
    "subnarratives_dfs = {}\n",
    "for subnarrative in subnarratives_list:\n",
    "    temp_df = annotations_df\n",
    "    temp_df['text'] = temp_df['id'].apply(lambda x: read_text(x))\n",
    "    temp_df['label'] = temp_df['subnarratives'].apply(lambda x: 1 if subnarrative in x else 0)\n",
    "    # keep only text and label column\n",
    "    temp_df = temp_df[['id','text', 'label']]\n",
    "    print('Attempting to create dataframe for subnarrative:', subnarrative)\n",
    "    subnarratives_dfs[subnarrative] = {\n",
    "        'df': temp_df,\n",
    "        'definition': get_subnarrative_definition(subnarrative),\n",
    "        'examples': get_subnarrative_examples(subnarrative)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the subnarratives dataframe by label to group positive and negative examples\n",
    "for subnarrative, data in subnarratives_dfs.items():\n",
    "    subnarratives_dfs[subnarrative]['df'] = data['df'].sort_values('label', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patching name='__init__', member=<function LLMLingua.__init__ at 0x7f583ed842c0>, patched=<function function.__call__ at 0x7f583ed84220>\n",
      "Patching name='compress_text', member=<function LLMLingua.compress_text at 0x7f583ed84360>, patched=<function function.__call__ at 0x7f583ed84540>\n"
     ]
    }
   ],
   "source": [
    "import autogen\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a classification model trained to do binary classification by detecting whether a given text is related to a specific subnarrative or not. \"\n",
    "    \"You have been trained to recognize the subnarrative: '{}'. \"\n",
    "    \"This subnarrative is defined as: {}. \"\n",
    "    \"Here are some examples of statements related to this subnarrative: {}. \"\n",
    "    \"If the text is related to the subnarrative, please respond with '1'. Otherwise, respond with '0'. Do not try to make sentences, just respond with '1' or '0'.\"\n",
    "    \"You are ONLY allowed to answer with '1' or '0' and NOTHING else.\"\n",
    "    \"Only answer with 1 if there are explicit and clear mentions of the subnarrative in the text. If you are slightly unsure, classify as 0.\"\n",
    ")\n",
    "\n",
    "user_prompt = (\n",
    "    \"Please classify the following text as related to the subnarrative '{}' or not. \"\n",
    "    \"Text : \\n ```{}```\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to create dataframe for narrative: Other\n",
      "Attempting to create dataframe for narrative: URW: Blaming the war on others rather than the invader\n",
      "Attempting to create dataframe for narrative: URW: Discrediting Ukraine\n",
      "Attempting to create dataframe for narrative: URW: Russia is the Victim\n",
      "Attempting to create dataframe for narrative: URW: Praise of Russia\n",
      "Attempting to create dataframe for narrative: URW: Overpraising the West\n",
      "Attempting to create dataframe for narrative: URW: Speculating war outcomes\n",
      "Attempting to create dataframe for narrative: URW: Discrediting the West, Diplomacy\n",
      "Attempting to create dataframe for narrative: URW: Negative Consequences for the West\n",
      "Attempting to create dataframe for narrative: URW: Distrust towards Media\n",
      "Attempting to create dataframe for narrative: URW: Amplifying war-related fears\n",
      "Attempting to create dataframe for narrative: URW: Hidden plots by secret schemes of powerful groups\n",
      "Attempting to create dataframe for narrative: CC: Criticism of climate policies\n",
      "Attempting to create dataframe for narrative: CC: Criticism of institutions and authorities\n",
      "Attempting to create dataframe for narrative: CC: Climate change is beneficial\n",
      "Attempting to create dataframe for narrative: CC: Downplaying climate change\n",
      "Attempting to create dataframe for narrative: CC: Questioning the measurements and science\n",
      "Attempting to create dataframe for narrative: CC: Criticism of climate movement\n",
      "Attempting to create dataframe for narrative: CC: Controversy about green technologies\n",
      "Attempting to create dataframe for narrative: CC: Hidden plots by secret schemes of powerful groups\n",
      "Attempting to create dataframe for narrative: CC: Amplifying Climate Fears\n",
      "Attempting to create dataframe for narrative: CC: Green policies are geopolitical instruments\n"
     ]
    }
   ],
   "source": [
    "narratives_dfs = {}\n",
    "\n",
    "for narrative in narratives_list:\n",
    "    temp_df = annotations_df\n",
    "    temp_df['text'] = temp_df['id'].apply(lambda x: read_text(x))\n",
    "    temp_df['label'] = temp_df['narratives'].apply(lambda x: 1 if narrative in x else 0)\n",
    "    # keep only text and label column\n",
    "    temp_df = temp_df[['text', 'label']]\n",
    "    print('Attempting to create dataframe for narrative:', narrative)\n",
    "    narratives_dfs[narrative] = {\n",
    "        'df': temp_df,\n",
    "        'definition': get_narrative_definition(narrative),\n",
    "        'examples': get_narrative_examples(narrative)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "narrative_system_prompt = (\n",
    "    \"You are a classification model trained to do binary classification by detecting whether a given text is related to a specific narrative or not. The only output you are allowed to give is '0' or '1' \"\n",
    "    \"You have been trained to recognize the narrative: '{}' \"\n",
    "    \"defined as: {}. \"\n",
    "    \"Here are some examples of statements related to this narrative: {}. \"\n",
    "    \"If the text is related to the narrative, you MUST respond with '1' only. Otherwise, you MUST with '0' only.\"\n",
    "    \"You are ONLY allowed to answer with '1' or '0' and NOTHING else.\"\n",
    "    \"Only answer with 1 if there are EXPLICIT and CLEAR mentions of the narrative in the text. Some text will be ambiguous so if you are slightly unsure, answer 0.\"\n",
    ")\n",
    "\n",
    "narrative_user_prompt = (\n",
    "    \"Please classify the following text as related to the narrative '{}' or not. \"\n",
    "    \"Text : \"\n",
    "    \"{}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_narrative_agents(narratives_list):\n",
    "    narrative_agents = []\n",
    "\n",
    "    for narrative in narratives_list:\n",
    "        agent = autogen.AssistantAgent(\n",
    "            name=\"Agent_\" + str(narratives_list.index(narrative)),\n",
    "            system_message= narrative_system_prompt.format(narrative, narratives_dfs[narrative]['definition'], narratives_dfs[narrative]['examples']),\n",
    "            llm_config={\n",
    "                \"config_list\": [\n",
    "                    {\n",
    "                        \"model\": \"gpt-4o\",\n",
    "                        \"api_key\": os.environ.get(\"OPENAI_API_KEY\")\n",
    "                    }\n",
    "                ],\n",
    "                'temperature': 0\n",
    "            }\n",
    "        )\n",
    "\n",
    "        agent.description = (\n",
    "            \"I am a classification model trained to do binary classification by detecting whether a given text is related to the following narrative: {}. \"\n",
    "            \"I will be looking for {}\"\n",
    "        ).format(get_narrative_short_name(narrative), get_narrative_definition(narrative))\n",
    "        narrative_agents.append(agent)\n",
    "    return narrative_agents\n",
    "\n",
    "narrative_agents = create_narrative_agents(narratives_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_agents(agent_list):\n",
    "    for agent in agent_list:\n",
    "        agent.reset()\n",
    "\n",
    "# reset_agents(narrative_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "narratives_user_proxy_agent = autogen.UserProxyAgent(\n",
    "    name=\"user\",\n",
    "    code_execution_config=False,\n",
    "    llm_config={\n",
    "        \"config_list\": [\n",
    "            {\n",
    "                \"model\": \"gpt-4o-mini\",\n",
    "                \"api_key\": os.environ.get(\"OPENAI_API_KEY\")\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    human_input_mode='NEVER'\n",
    ")\n",
    "\n",
    "allowed_transitions = {}\n",
    "\n",
    "for agent in narrative_agents:\n",
    "    allowed_transitions[agent] = [narratives_user_proxy_agent]\n",
    "\n",
    "narratives_group_chat = autogen.GroupChat(\n",
    "    agents= [narratives_user_proxy_agent] + narrative_agents,\n",
    "    messages=[],\n",
    "    max_round=6,\n",
    "    send_introductions=True,\n",
    "    allowed_or_disallowed_speaker_transitions=allowed_transitions,\n",
    "    speaker_transitions_type=\"disallowed\",\n",
    ")\n",
    "\n",
    "narratives_manager = autogen.GroupChatManager(\n",
    "    groupchat=narratives_group_chat,\n",
    "    llm_config = {\n",
    "        \"config_list\": [\n",
    "            {\n",
    "                \"model\": \"gpt-4o\",\n",
    "                \"api_key\": os.environ.get(\"OPENAI_API_KEY\")\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(user_proxy_agent, manager, df, n = 5):\n",
    "    histories = []\n",
    "    for i in range(n):\n",
    "        chat_result = user_proxy_agent.initiate_chat(\n",
    "            manager,\n",
    "            message=\"Here is the text that needs to be classified: \\n```{}```\\n ### \\n You are ONLY allowed to reply with '0' or '1'\".format(df['text'].iloc[i]),\n",
    "            summary_method='reflection_with_llm'\n",
    "        )\n",
    "        histories.append({\n",
    "            'chat_result': chat_result.chat_history,\n",
    "            'file': df['id'].iloc[i]\n",
    "        })\n",
    "        narratives_user_proxy_agent.reset()\n",
    "        reset_agents(narrative_agents)\n",
    "        print('Completed ', i+1, ' out of ', n, ' texts')\n",
    "    return histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_recognized_narratives(chat_history):\n",
    "    recognized_narratives = []\n",
    "    for message in chat_history:\n",
    "        if message['name'] == 'user' or message['name'] == 'chat_manager':\n",
    "            continue\n",
    "        index = int(message['name'].split('_')[-1])\n",
    "        if message['content'] == '1':\n",
    "            recognized_narratives.append(narratives_list[index])\n",
    "    if len(recognized_narratives) == 0:\n",
    "        recognized_narratives.append('Other')\n",
    "\n",
    "    if len(recognized_narratives) > 1 and 'Other' in recognized_narratives:\n",
    "        recognized_narratives.remove('Other')\n",
    "    return recognized_narratives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(extracted_narratives, true_narratives):\n",
    "    if len(extracted_narratives) == 0:\n",
    "        return 0\n",
    "    return sum([1 for narrative in extracted_narratives if narrative in true_narratives]) / len(extracted_narratives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = classify(narratives_user_proxy_agent, narratives_manager, annotations_df, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy for each history\n",
    "import pprint\n",
    "accuracies = []\n",
    "for history in histories:\n",
    "    extracted_narratives = extract_recognized_narratives(history['chat_result'])\n",
    "    true_narratives = annotations_df[annotations_df['id'] == history['file']]['narratives'].values[0]\n",
    "    accuracies.append(get_accuracy(extracted_narratives, true_narratives))\n",
    "\n",
    "pprint.pprint(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_annotations_df(histories, base_path='data/EN/raw-documents'):\n",
    "    annotations = []\n",
    "    for history in histories:\n",
    "        extracted_narratives = extract_recognized_narratives(history['chat_result'])\n",
    "        # Remove 'Other' if there are additional narratives\n",
    "        if 'Other' in extracted_narratives and len(extracted_narratives) > 1:\n",
    "            extracted_narratives = [narrative for narrative in extracted_narratives if narrative != 'Other']\n",
    "        \n",
    "        extracted_narratives = list(set(extracted_narratives))\n",
    "        \n",
    "        annotations.append({\n",
    "            'id': history['file'],\n",
    "            'text': read_text(history['file'], base_path),\n",
    "            'narratives': extracted_narratives\n",
    "        })\n",
    "    df = pd.DataFrame(annotations)\n",
    "    df['narratives'] = df['narratives'].apply(lambda x: ';'.join(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_annotation_df = create_annotations_df(histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subnarrative_agents(subnarratives_list):\n",
    "    subnarrative_agents = {}\n",
    "    for subnarrative in subnarratives_list:\n",
    "        agent = autogen.AssistantAgent(\n",
    "            name=\"Agent_\" + str(subnarratives_list.index(subnarrative)),\n",
    "            system_message= system_prompt.format(subnarrative, subnarratives_dfs[subnarrative]['definition'], subnarratives_dfs[subnarrative]['examples']),\n",
    "            llm_config={\n",
    "                \"config_list\": [\n",
    "                    {\n",
    "                        \"base_url\": \"https://api.deepseek.com\",\n",
    "                        \"model\": \"deepseek-chat\",\n",
    "                        \"api_key\": os.environ.get(\"DEEPSEEK_API_KEY\")\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        agent.description = 'I am a classification model trained to do binary classification by detecting whether a given text is related to the following subnarrative: {}'.format(get_subnarrative_short_name(subnarrative))\n",
    "        subnarrative_agents[subnarrative] = agent\n",
    "\n",
    "    return subnarrative_agents\n",
    "\n",
    "def create_group_chat(agent_list):\n",
    "    user_proxy_agent = autogen.UserProxyAgent(\n",
    "        name=\"user\",\n",
    "        code_execution_config=False,\n",
    "        llm_config={\n",
    "            \"config_list\": [\n",
    "                {\n",
    "                    \"base_url\": \"https://api.deepseek.com\",\n",
    "                    \"model\": \"deepseek-chat\",\n",
    "                    \"api_key\": os.environ.get(\"DEEPSEEK_API_KEY\")\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        human_input_mode='NEVER'\n",
    "    )\n",
    "    \n",
    "    allowed_transitions = {}\n",
    "    for agent in agent_list:\n",
    "        allowed_transitions[agent] = [user_proxy_agent]\n",
    "\n",
    "    group_chat = autogen.GroupChat(\n",
    "        agents= [user_proxy_agent] + agent_list,\n",
    "        messages=[],\n",
    "        max_round=len(agent_list) + 1,\n",
    "        send_introductions=True,\n",
    "        allowed_or_disallowed_speaker_transitions=allowed_transitions,\n",
    "        speaker_transitions_type='disallowed',\n",
    "    )\n",
    "\n",
    "    manager = autogen.GroupChatManager(\n",
    "        groupchat=group_chat,\n",
    "        llm_config = {\n",
    "            \"config_list\": [\n",
    "                {\n",
    "                    \"base_url\": \"https://api.deepseek.com\",\n",
    "                    \"model\": \"deepseek-chat\",\n",
    "                    \"api_key\": os.environ.get(\"DEEPSEEK_API_KEY\")\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        system_message=\"You are a group chat manager. You are asked to give the classification task to the agents that look relevant to the topic\"\n",
    "    )\n",
    "\n",
    "    print('Created group chat with the following agents: ', agent_list)\n",
    "    return group_chat, manager, user_proxy_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group subnarrative agents by narrative\n",
    "\n",
    "def group_subnarrative_agents_by_narrative(subnarrative_agents):\n",
    "    grouped_agents = {}\n",
    "    for subnarrative, agent in subnarrative_agents.items():\n",
    "        if subnarrative == 'Other':\n",
    "            continue\n",
    "        narrative = subnarrative.split(': ')[0] + ': ' + subnarrative.split(': ')[1]\n",
    "        if narrative not in grouped_agents:\n",
    "            grouped_agents[narrative] = []\n",
    "        grouped_agents[narrative].append(agent)\n",
    "    return grouped_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "subnarrative_agents = create_subnarrative_agents(subnarratives_list)\n",
    "grouped_agents = group_subnarrative_agents_by_narrative(subnarrative_agents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_group_chat_for_narrative(narrative):\n",
    "    return create_group_chat(grouped_agents[narrative])\n",
    "\n",
    "def extract_recognized_subnarratives(chat_history):\n",
    "    recognized_subnarratives = []\n",
    "    for message in chat_history:\n",
    "        if message['name'] == 'user' or message['name'] == 'chat_manager':\n",
    "            continue\n",
    "        index = int(message['name'].split('_')[-1])\n",
    "        if message['content'] == '1':\n",
    "            recognized_subnarratives.append(subnarratives_list[index])\n",
    "    if len(recognized_subnarratives) == 0:\n",
    "        recognized_subnarratives.append('Other')\n",
    "\n",
    "    if len(recognized_subnarratives) > 1 and 'Other' in recognized_subnarratives:\n",
    "        recognized_subnarratives.remove('Other')\n",
    "    return recognized_subnarratives\n",
    "\n",
    "def extract_subnarratives_for_one_narrative(narrative, text):\n",
    "    group_chat, manager, user_proxy_agent = create_group_chat_for_narrative(narrative)\n",
    "    # We get the subnarratives that belong to the narrative\n",
    "    chat_result = user_proxy_agent.initiate_chat(\n",
    "        manager,\n",
    "        message=\"Here is the text that needs to be classified: \\n```{}```\\nYou are ONLY allowed to reply with '0' or '1'\".format(text),\n",
    "        summary_method='reflection_with_llm'\n",
    "    )\n",
    "    return extract_recognized_subnarratives(chat_result.chat_history)\n",
    "\n",
    "def extract_subnarratives_for_narratives(narratives_list, text):\n",
    "    # if the list only has one element that is 'Other', return 'Other'\n",
    "    if len(narratives_list) == 1 and narratives_list[0] == 'Other':\n",
    "        return ['Other']\n",
    "    subnarratives = []\n",
    "    for narrative in narratives_list:\n",
    "        subnarratives.append(extract_subnarratives_for_one_narrative(narrative, text))\n",
    "    return subnarratives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subnarratives_for_df(df):\n",
    "    # Create a new list to store subnarratives for each row\n",
    "    subnarratives = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        narratives = row['narratives'].split(';')  # Split narratives by ';'\n",
    "        subnarratives.append(extract_subnarratives_for_narratives(narratives, row['text']))  # Extract subnarratives\n",
    "        \n",
    "\n",
    "    # Add a new column to the DataFrame with the extracted subnarratives\n",
    "    df['subnarratives'] = subnarratives\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_subnarratives_for_df(test_annotation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat arrays\n",
    "def concat_arrays(arr):\n",
    "    if len(arr) == 0:\n",
    "        return []\n",
    "    if len(arr) == 1:\n",
    "        return arr[0]\n",
    "    return [item for sublist in arr for item in sublist]\n",
    "\n",
    "def update_narratives_for_only_other(df):\n",
    "    \"\"\"\n",
    "    Updates the 'narratives' column to 'Other' if the 'subnarratives' column contains only 'Other'.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame with 'narratives' and 'subnarratives' columns.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The updated DataFrame.\n",
    "    \"\"\"\n",
    "    def update_row(row):\n",
    "        # Check if subnarratives consist only of 'Other'\n",
    "        if row['subnarratives'] == ['Other']:\n",
    "            row['narratives'] = 'Other'\n",
    "        return row\n",
    "\n",
    "    # Apply the update to each row\n",
    "    return df.apply(update_row, axis=1)\n",
    "\n",
    "def remove_parasit_other(df):\n",
    "    \"\"\"\n",
    "    Removes 'Other' from the 'subnarratives' column if it is the only element in the list.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame with 'subnarratives' column.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The updated DataFrame.\n",
    "    \"\"\"\n",
    "    def remove_other(row):\n",
    "        if len(row) > 1 and 'Other' in row:\n",
    "            row.remove('Other')\n",
    "        return row\n",
    "\n",
    "    # Apply the removal to the 'subnarratives' column\n",
    "    df['subnarratives'] = df['subnarratives'].apply(remove_other)\n",
    "    return df\n",
    "\n",
    "def flatten_and_deduplicate_subnarratives(df, column_name):\n",
    "    \"\"\"\n",
    "    Flattens and deduplicates list of lists in the specified column of the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    column_name (str): The name of the column to process.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The updated DataFrame with flattened and deduplicated lists.\n",
    "    \"\"\"\n",
    "    def process_row(value):\n",
    "        if isinstance(value, list):  # Check if the value is a list\n",
    "            # Flatten and deduplicate if it's a list of lists\n",
    "            flattened = set()\n",
    "            for item in value:\n",
    "                if isinstance(item, list):\n",
    "                    flattened.update(item)  # Add elements from inner lists\n",
    "                else:\n",
    "                    flattened.add(item)  # Add single elements\n",
    "            return list(flattened)  # Return as a unique list\n",
    "        return value  # Return as-is if not a list\n",
    "\n",
    "    # Apply the processing function to the specified column\n",
    "    df[column_name] = df[column_name].apply(process_row)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_to_right_format(df):\n",
    "    df = flatten_and_deduplicate_subnarratives(df, 'subnarratives')\n",
    "    df = remove_parasit_other(df)\n",
    "    df = update_narratives_for_only_other(df)\n",
    "    # join subnarratives\n",
    "    df['subnarratives'] = df['subnarratives'].apply(lambda x: ';'.join(x))\n",
    "    # keep only id, narratives and subnarratives\n",
    "    df = df[['id', 'narratives', 'subnarratives']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_classification_accuracy(predictions_df, ground_truth_df):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy of subnarrative classification by comparing predictions with ground truth.\n",
    "\n",
    "    Parameters:\n",
    "    predictions_df (pd.DataFrame): DataFrame with your classification results, including 'id' and 'subnarratives'.\n",
    "    ground_truth_df (pd.DataFrame): DataFrame with the ground truth classifications, including 'id' and 'subnarratives'.\n",
    "\n",
    "    Returns:\n",
    "    float: Overall accuracy (average IoU) for all rows.\n",
    "    pd.DataFrame: DataFrame with per-row IoU scores.\n",
    "    \"\"\"\n",
    "    def calculate_iou(predicted, true):\n",
    "        # Convert to sets for comparison\n",
    "        predicted_set = set(predicted)\n",
    "        true_set = set(true)\n",
    "\n",
    "        # Handle edge case: both are empty\n",
    "        if not predicted_set and not true_set:\n",
    "            return 1.0\n",
    "\n",
    "        # Intersection over union (IoU)\n",
    "        intersection = len(predicted_set & true_set)\n",
    "        union = len(predicted_set | true_set)\n",
    "        return intersection / union if union > 0 else 0.0\n",
    "\n",
    "    # Merge predictions and ground truth on 'id'\n",
    "    merged_df = predictions_df.merge(ground_truth_df, on='id', suffixes=('_predicted', '_true'))\n",
    "\n",
    "    # Calculate IoU for each row\n",
    "    merged_df['iou'] = merged_df.apply(\n",
    "        lambda row: calculate_iou(row['subnarratives_predicted'], row['subnarratives_true']), axis=1\n",
    "    )\n",
    "\n",
    "    # Calculate overall accuracy (average IoU)\n",
    "    overall_accuracy = merged_df['iou'].mean()\n",
    "\n",
    "    return overall_accuracy, merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first 10 rows of the annotations_df\n",
    "#test_annotation_df['narratives'] = test_annotation_df['narratives'].apply(lambda x: x.split(';'))\n",
    "#test_annotation_df['subnarratives'] = test_annotation_df['subnarratives'].apply(lambda x: x.split(';'))\n",
    "truth_annotation_df = annotations_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_accuracy, merged_df = calculate_classification_accuracy(test_annotation_df, truth_annotation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(folder):\n",
    "    files = []\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith('.txt'):\n",
    "            files.append(file)\n",
    "    return files\n",
    "\n",
    "def create_text_df(files):\n",
    "    texts = []\n",
    "    for file in files:\n",
    "        texts.append({\n",
    "            'id': file,\n",
    "            'text': read_text(file, base_path='devset/EN/subtask-2-documents')\n",
    "        })\n",
    "    return pd.DataFrame(texts)\n",
    "\n",
    "files = load_files('devset/EN/subtask-2-documents')\n",
    "dev_df = create_text_df(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_agents(narrative_agents)\n",
    "for subnarrative in subnarratives_list:\n",
    "    subnarrative_agents[subnarrative].reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "narrative_agents = create_narrative_agents(narratives_list)\n",
    "#histories = classify(narratives_user_proxy_agent, narratives_manager, dev_df, len(dev_df))\n",
    "#dev_df = create_annotations_df(histories, base_path='devset/EN/subtask-2-documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "subnarrative_agents = []\n",
    "subnarrative_agents = create_subnarrative_agents(subnarratives_list)\n",
    "grouped_agents = group_subnarrative_agents_by_narrative(subnarrative_agents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_subnarratives_for_df(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = get_df_to_right_format(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the dataframe to a txt file with no headers\n",
    "# keep only id, narratives and subnarratives\n",
    "dev_df = dev_df[['id', 'narratives', 'subnarratives']]\n",
    "dev_df.to_csv('devset/EN/subtask-2-annotations.txt', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions_for_texts(folder):\n",
    "    def load_files(folder):\n",
    "        files = []\n",
    "        for file in os.listdir(folder):\n",
    "            if file.endswith('.txt'):\n",
    "                files.append(file)\n",
    "        return files\n",
    "\n",
    "    def create_text_df(files):\n",
    "        texts = []\n",
    "        for file in files:\n",
    "            texts.append({\n",
    "                'id': file,\n",
    "                'text': read_text(file, base_path=folder)\n",
    "            })\n",
    "        return pd.DataFrame(texts)\n",
    "\n",
    "    files = load_files(folder)\n",
    "    predictions_df = create_text_df(files)\n",
    "    narrative_agents = create_narrative_agents(narratives_list)\n",
    "    histories = classify(narratives_user_proxy_agent, narratives_manager, predictions_df, len(predictions_df))\n",
    "    predictions_df = create_annotations_df(histories, base_path=folder)\n",
    "\n",
    "    subnarrative_agents = []\n",
    "    subnarrative_agents = create_subnarrative_agents(subnarratives_list)\n",
    "    grouped_agents = group_subnarrative_agents_by_narrative(subnarrative_agents)\n",
    "    \n",
    "    get_subnarratives_for_df(predictions_df)\n",
    "    get_df_to_right_format(predictions_df)\n",
    "    return predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser\u001b[0m (to chat_manager):\n",
      "\n",
      "Here is the text that needs to be classified: \n",
      "```Bidenâ€™s green policies are making housing no longer affordable for the average American \n",
      "\n",
      " This is according to Edward Ring, an author and a senior fellow with the conservative think tank, the California Policy Center, who warned of the far-reaching consequences of Biden's \"globalist green agenda.\"\n",
      "\n",
      "\"To allegedly save us all from a 'climate crisis,' development and use of oil, natural gas and coal is being halted,\" wrote Ring for American Greatness, while at the same time pointing out that the development of renewable energy sources like hydroelectric and nuclear power plants is also being slowed down, if not completely stopped.\n",
      "\n",
      "Ring added that newer energy sources will come from more inefficient and expensive wind, solar and biomass energy generation. These are far more destructive to the environment, require the extraction and importing of more raw materials for their construction and maintenance \"and cannot possibly deliver the amount of energy the nation â€“ or the world â€“ requires to prosper.\"\n",
      "\n",
      "\"The consequences of \"green\" policies are the primary reason why most Americans can no longer afford to own homes or pay rent, buy gasoline or pay their utility bills,\" wrote Ring. \"And these elevated prices for essentials factor into price increases for everything else. How will doing this make America strong enough to withstand a prolonged military conflict with peer adversaries?\"\n",
      "\n",
      "Perhaps the most devastating impact on Americans of these green energy policies is the fact that housing is becoming more and more unaffordable for Americans.\n",
      "\n",
      "We are building the infrastructure of human freedom and empowering people to be informed, healthy and aware. Explore our decentralized, peer-to-peer, uncensorable Brighteon.io free speech platform here. Learn about our free, downloadable generative AI tools at Brighteon.AI. Every purchase at HealthRangerStore.com helps fund our efforts to build and share more tools for empowering humanity with knowledge and abundance.\n",
      "\n",
      "Biden's Department of Energy has announced a $400 million program meant to go to state and local governments to incentivize them to implement new building codes that \"lower greenhouse gas emissions\" and fight the so-called \"climate crisis.\" (Related: Interest rate for 30-year fixed mortgage rises to 8.45% â€“ the highest it has been since 2000.)\n",
      "\n",
      "More than half of this endowment will go to implementing a universal \"energy conservation code\" that will compel homeowners to spend as much as $31,000 to renovate their homes to be in compliance with this directive.\n",
      "\n",
      "The remainder of the fund will go toward implementing \"zero energy\" building codes, which will require residential buildings to install small-scale renewable energy generators \"to achieve zero-net carbon.\"\n",
      "\n",
      "This additional spending for owners of houses and residential buildings comes at a period in the United States when families are already struggling to afford a home. The median home price in the U.S. already rose to its second-highest-ever price of $414,000 in July. Forcing developers to build new houses and apartments in such a way as to comply with green building codes.\n",
      "\n",
      "State and local governments that take advantage of the Biden administration's funding for implementing green building codes would further be spurred to eventually phase out the use of natural gas in homes and residential buildings â€“ which could make energy costs even more expensive as cheap natural gas disappears.\n",
      "\n",
      "Watch this video of financial and economic expert Gregory Mannarino warning Americans to prepare for the biggest housing crash in the history of the world.\n",
      "\n",
      "This video is from the High Hopes channel on Brighteon.com.\n",
      "\n",
      "IRONIC? Tesla is building the world's largest supercharger station, but it's powered by a diesel power plant.\n",
      "\n",
      "BLINDED by MONEY: Wind farms are KILLING eagles and whales, but the green cult ignores reality as it pockets millions in donations.\n",
      "\n",
      "John Kerry admits NUCLEAR ENERGY is necessary but continues to push for wind and solar instead.\n",
      "\n",
      "Almost half of all young adults in the United States are living with their parents.\n",
      "\n",
      "\"Green energy\" mining is polluting rivers and farmland at an unsustainable pace, leaving 23 million people exposed to toxic waste.\n",
      "\n",
      "Sources include:```\n",
      " ### \n",
      " You are ONLY allowed to reply with '0' or '1'\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Authentication Fails (no such user)', 'type': 'authentication_error', 'param': None, 'code': 'invalid_request_error'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_df \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_predictions_for_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdevset/EN/subtask-2-documents\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[74], line 21\u001b[0m, in \u001b[0;36mgenerate_predictions_for_texts\u001b[0;34m(folder)\u001b[0m\n\u001b[1;32m     19\u001b[0m predictions_df \u001b[38;5;241m=\u001b[39m create_text_df(files)\n\u001b[1;32m     20\u001b[0m narrative_agents \u001b[38;5;241m=\u001b[39m create_narrative_agents(narratives_list)\n\u001b[0;32m---> 21\u001b[0m histories \u001b[38;5;241m=\u001b[39m \u001b[43mclassify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnarratives_user_proxy_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnarratives_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpredictions_df\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m predictions_df \u001b[38;5;241m=\u001b[39m create_annotations_df(histories, base_path\u001b[38;5;241m=\u001b[39mfolder)\n\u001b[1;32m     24\u001b[0m subnarrative_agents \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[70], line 4\u001b[0m, in \u001b[0;36mclassify\u001b[0;34m(user_proxy_agent, manager, df, n)\u001b[0m\n\u001b[1;32m      2\u001b[0m histories \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[0;32m----> 4\u001b[0m     chat_result \u001b[38;5;241m=\u001b[39m \u001b[43muser_proxy_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHere is the text that needs to be classified: \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m```\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m```\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m ### \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m You are ONLY allowed to reply with \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m or \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43msummary_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreflection_with_llm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     histories\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchat_result\u001b[39m\u001b[38;5;124m'\u001b[39m: chat_result\u001b[38;5;241m.\u001b[39mchat_history,\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m'\u001b[39m: df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[i]\n\u001b[1;32m     12\u001b[0m     })\n\u001b[1;32m     13\u001b[0m     narratives_user_proxy_agent\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:1125\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1124\u001b[0m         msg2send \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_init_message(message, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1125\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg2send\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1126\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_summarize_chat(\n\u001b[1;32m   1127\u001b[0m     summary_method,\n\u001b[1;32m   1128\u001b[0m     summary_args,\n\u001b[1;32m   1129\u001b[0m     recipient,\n\u001b[1;32m   1130\u001b[0m     cache\u001b[38;5;241m=\u001b[39mcache,\n\u001b[1;32m   1131\u001b[0m )\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mself\u001b[39m, recipient]:\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:817\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    815\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient, is_sending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 817\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    821\u001b[0m     )\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:925\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    924\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 925\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(reply, sender, silent\u001b[38;5;241m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:2066\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, **kwargs)\u001b[0m\n\u001b[1;32m   2064\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   2065\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[0;32m-> 2066\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2067\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[1;32m   2068\u001b[0m         log_event(\n\u001b[1;32m   2069\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2070\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreply_func_executed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2074\u001b[0m             reply\u001b[38;5;241m=\u001b[39mreply,\n\u001b[1;32m   2075\u001b[0m         )\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/groupchat.py:1169\u001b[0m, in \u001b[0;36mGroupChatManager.run_chat\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;66;03m# select the next speaker\u001b[39;00m\n\u001b[0;32m-> 1169\u001b[0m     speaker \u001b[38;5;241m=\u001b[39m \u001b[43mgroupchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_speaker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspeaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m silent:\n\u001b[1;32m   1171\u001b[0m         iostream \u001b[38;5;241m=\u001b[39m IOStream\u001b[38;5;241m.\u001b[39mget_default()\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/groupchat.py:570\u001b[0m, in \u001b[0;36mGroupChat.select_speaker\u001b[0;34m(self, last_speaker, selector)\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_agent(last_speaker)\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# auto speaker selection with 2-agent chat\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_auto_select_speaker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_speaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/groupchat.py:746\u001b[0m, in \u001b[0;36mGroupChat._auto_select_speaker\u001b[0;34m(self, last_speaker, selector, messages, agents)\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_speaker_selection_transforms\u001b[38;5;241m.\u001b[39madd_to_agent(speaker_selection_agent)\n\u001b[1;32m    745\u001b[0m \u001b[38;5;66;03m# Run the speaker selection chat\u001b[39;00m\n\u001b[0;32m--> 746\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mchecking_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeaker_selection_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# don't use caching for the speaker selection chat\u001b[39;49;00m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_turns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_attempts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Limiting the chat to the number of attempts, including the initial one\u001b[39;49;00m\n\u001b[1;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclear_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_speaker_auto_verbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Base silence on the verbose attribute\u001b[39;49;00m\n\u001b[1;32m    754\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_speaker_selection_result(result, last_speaker, agents)\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:1118\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m msg2send \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1117\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1118\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg2send\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_chat(recipient, clear_history)\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:817\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    815\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient, is_sending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 817\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    821\u001b[0m     )\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:925\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    924\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 925\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(reply, sender, silent\u001b[38;5;241m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:2066\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, **kwargs)\u001b[0m\n\u001b[1;32m   2064\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   2065\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[0;32m-> 2066\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2067\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[1;32m   2068\u001b[0m         log_event(\n\u001b[1;32m   2069\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2070\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreply_func_executed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2074\u001b[0m             reply\u001b[38;5;241m=\u001b[39mreply,\n\u001b[1;32m   2075\u001b[0m         )\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:1443\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m messages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1442\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_oai_messages[sender]\n\u001b[0;32m-> 1443\u001b[0m extracted_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_oai_reply_from_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_oai_system_message\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient_cache\u001b[49m\n\u001b[1;32m   1445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, extracted_response)\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:1462\u001b[0m, in \u001b[0;36mConversableAgent._generate_oai_reply_from_client\u001b[0;34m(self, llm_client, messages, cache)\u001b[0m\n\u001b[1;32m   1459\u001b[0m         all_messages\u001b[38;5;241m.\u001b[39mappend(message)\n\u001b[1;32m   1461\u001b[0m \u001b[38;5;66;03m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[0;32m-> 1462\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1466\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1467\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1468\u001b[0m extracted_response \u001b[38;5;241m=\u001b[39m llm_client\u001b[38;5;241m.\u001b[39mextract_text_or_completion_object(response)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/oai/client.py:964\u001b[0m, in \u001b[0;36mOpenAIWrapper.create\u001b[0;34m(self, **config)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    963\u001b[0m     request_ts \u001b[38;5;241m=\u001b[39m get_current_ts()\n\u001b[0;32m--> 964\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m APITimeoutError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    966\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/oai/client.py:468\u001b[0m, in \u001b[0;36mOpenAIClient.create\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_reasoning_model_params(params)\n\u001b[1;32m    467\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 468\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_or_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# remove the system_message from the response and add it in the prompt at the start.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_o1:\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/oai/client.py:302\u001b[0m, in \u001b[0;36mOpenAIClient._handle_openai_bad_request_error.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mBadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    304\u001b[0m         response_json \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/openai/resources/chat/completions.py:850\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    810\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    847\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    848\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    849\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 850\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/openai/_base_client.py:1283\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1271\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1279\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1280\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1281\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1282\u001b[0m     )\n\u001b[0;32m-> 1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/openai/_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/openai/_base_client.py:1064\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1063\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1064\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1067\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1068\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1072\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1073\u001b[0m )\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Authentication Fails (no such user)', 'type': 'authentication_error', 'param': None, 'code': 'invalid_request_error'}}"
     ]
    }
   ],
   "source": [
    "test_df = generate_predictions_for_texts('devset/EN/subtask-2-documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df[['id', 'narratives', 'subnarratives']]\n",
    "test_df['subnarratives'] = test_df['subnarratives'].apply(lambda x: ';'.join(x))\n",
    "test_df.to_csv('devset/EN/subtask-2-annotations-4o-mini.txt', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def load_annotation_dicts(filepath):\n",
    "    narratives_dict = {}\n",
    "    subnarratives_dict = {}\n",
    "    \n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\"\\t\")\n",
    "        for row in reader:\n",
    "            if len(row) < 3:\n",
    "                continue  # Skip malformed lines\n",
    "            \n",
    "            file_id = row[0].strip()\n",
    "            # Split semicolon-separated labels\n",
    "            narratives = [x.strip() for x in row[1].split(\";\") if x.strip()]\n",
    "            subnarratives = [x.strip() for x in row[2].split(\";\") if x.strip()]\n",
    "            \n",
    "            narratives_dict[file_id] = set(narratives)\n",
    "            subnarratives_dict[file_id] = set(subnarratives)\n",
    "    \n",
    "    return narratives_dict, subnarratives_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def compute_multilabel_confusion_matrices(pred_dict, gold_dict):\n",
    "    \"\"\"\n",
    "    pred_dict: {file_id: set_of_labels}\n",
    "    gold_dict: {file_id: set_of_labels}\n",
    "\n",
    "    Returns a dict: { label: confusion_matrix(2x2) }\n",
    "                    where confusion_matrix is [[TN, FP],\n",
    "                                               [FN, TP]]\n",
    "    \"\"\"\n",
    "    # 1) Collect all unique labels\n",
    "    all_labels = set()\n",
    "    for file_id in pred_dict:\n",
    "        all_labels.update(pred_dict[file_id])\n",
    "    for file_id in gold_dict:\n",
    "        all_labels.update(gold_dict[file_id])\n",
    "    \n",
    "    # 2) Prepare the confusion matrices\n",
    "    label_to_confmat = {}\n",
    "\n",
    "    # We'll ensure we iterate over all file IDs (union of both keys).\n",
    "    all_file_ids = set(pred_dict.keys()).union(gold_dict.keys())\n",
    "    \n",
    "    for label in sorted(all_labels):\n",
    "        # Build y_true and y_pred for this label\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        for file_id in all_file_ids:\n",
    "            gold_set = gold_dict.get(file_id, set())\n",
    "            pred_set = pred_dict.get(file_id, set())\n",
    "            \n",
    "            # 1 if this label is in the gold set, else 0\n",
    "            y_true.append(1 if label in gold_set else 0)\n",
    "            # 1 if this label is predicted, else 0\n",
    "            y_pred.append(1 if label in pred_set else 0)\n",
    "        \n",
    "        # 3) Compute scikit-learn confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        # cm is [[TN, FP], [FN, TP]]\n",
    "        \n",
    "        label_to_confmat[label] = cm\n",
    "    \n",
    "    return label_to_confmat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Confusion Matrices for Narratives ===\n",
      "CC: Amplifying Climate Fears => TN=39, FP=2, FN=0, TP=0\n",
      "CC: Climate change is beneficial => TN=40, FP=0, FN=0, TP=1\n",
      "CC: Controversy about green technologies => TN=34, FP=5, FN=0, TP=2\n",
      "CC: Criticism of climate movement => TN=25, FP=8, FN=1, TP=7\n",
      "CC: Criticism of climate policies => TN=24, FP=14, FN=2, TP=1\n",
      "CC: Criticism of institutions and authorities => TN=27, FP=6, FN=3, TP=5\n",
      "CC: Downplaying climate change => TN=36, FP=3, FN=2, TP=0\n",
      "CC: Green policies are geopolitical instruments => TN=37, FP=1, FN=1, TP=2\n",
      "CC: Hidden plots by secret schemes of powerful groups => TN=36, FP=1, FN=4, TP=0\n",
      "CC: Questioning the measurements and science => TN=36, FP=1, FN=1, TP=3\n",
      "Other => TN=28, FP=2, FN=6, TP=5\n",
      "URW: Amplifying war-related fears => TN=36, FP=2, FN=3, TP=0\n",
      "URW: Blaming the war on others rather than the invader => TN=35, FP=0, FN=2, TP=4\n",
      "URW: Discrediting Ukraine => TN=34, FP=0, FN=2, TP=5\n",
      "URW: Discrediting the West, Diplomacy => TN=32, FP=0, FN=4, TP=5\n",
      "URW: Distrust towards Media => TN=37, FP=0, FN=2, TP=2\n",
      "URW: Hidden plots by secret schemes of powerful groups => TN=39, FP=2, FN=0, TP=0\n",
      "URW: Negative Consequences for the West => TN=34, FP=6, FN=1, TP=0\n",
      "URW: Overpraising the West => TN=40, FP=0, FN=1, TP=0\n",
      "URW: Praise of Russia => TN=39, FP=0, FN=2, TP=0\n",
      "URW: Russia is the Victim => TN=39, FP=0, FN=2, TP=0\n",
      "URW: Speculating war outcomes => TN=35, FP=2, FN=3, TP=1\n",
      "\n",
      "=== Confusion Matrices for Subnarratives ===\n",
      "CC: Amplifying Climate Fears: Amplifying existing fears of global warming => TN=40, FP=1, FN=0, TP=0\n",
      "CC: Amplifying Climate Fears: Doomsday scenarios for humans => TN=39, FP=2, FN=0, TP=0\n",
      "CC: Amplifying Climate Fears: Earth will be uninhabitable soon => TN=40, FP=1, FN=0, TP=0\n",
      "CC: Climate change is beneficial: CO2 is beneficial => TN=40, FP=0, FN=0, TP=1\n",
      "CC: Controversy about green technologies: Other => TN=38, FP=2, FN=0, TP=1\n",
      "CC: Controversy about green technologies: Renewable energy is costly => TN=40, FP=0, FN=0, TP=1\n",
      "CC: Controversy about green technologies: Renewable energy is dangerous => TN=39, FP=1, FN=0, TP=1\n",
      "CC: Controversy about green technologies: Renewable energy is unreliable => TN=40, FP=1, FN=0, TP=0\n",
      "CC: Criticism of climate movement: Ad hominem attacks on key activists => TN=37, FP=1, FN=1, TP=2\n",
      "CC: Criticism of climate movement: Climate movement is alarmist => TN=36, FP=1, FN=1, TP=3\n",
      "CC: Criticism of climate movement: Climate movement is corrupt => TN=37, FP=1, FN=2, TP=1\n",
      "CC: Criticism of climate movement: Other => TN=34, FP=3, FN=3, TP=1\n",
      "CC: Criticism of climate policies: Climate policies are only for profit => TN=37, FP=3, FN=1, TP=0\n",
      "CC: Criticism of climate policies: Climate policies have negative impact on the economy => TN=40, FP=0, FN=0, TP=1\n",
      "CC: Criticism of climate policies: Other => TN=39, FP=1, FN=1, TP=0\n",
      "CC: Criticism of institutions and authorities: Criticism of international entities => TN=38, FP=1, FN=1, TP=1\n",
      "CC: Criticism of institutions and authorities: Criticism of national governments => TN=37, FP=1, FN=2, TP=1\n",
      "CC: Criticism of institutions and authorities: Criticism of political organizations and figures => TN=33, FP=2, FN=2, TP=4\n",
      "CC: Criticism of institutions and authorities: Other => TN=36, FP=4, FN=1, TP=0\n",
      "CC: Downplaying climate change: Human activities do not impact climate change => TN=39, FP=0, FN=2, TP=0\n",
      "CC: Downplaying climate change: Ice is not melting => TN=40, FP=1, FN=0, TP=0\n",
      "CC: Downplaying climate change: Other => TN=39, FP=1, FN=1, TP=0\n",
      "CC: Downplaying climate change: Weather suggests the trend is global cooling => TN=40, FP=1, FN=0, TP=0\n",
      "CC: Green policies are geopolitical instruments: Climate-related international relations are abusive/exploitative => TN=39, FP=0, FN=1, TP=1\n",
      "CC: Green policies are geopolitical instruments: Other => TN=39, FP=1, FN=1, TP=0\n",
      "CC: Hidden plots by secret schemes of powerful groups: Blaming global elites => TN=39, FP=0, FN=2, TP=0\n",
      "CC: Hidden plots by secret schemes of powerful groups: Climate agenda has hidden motives => TN=40, FP=0, FN=1, TP=0\n",
      "CC: Hidden plots by secret schemes of powerful groups: Other => TN=40, FP=0, FN=1, TP=0\n",
      "CC: Questioning the measurements and science: Data shows no temperature increase => TN=39, FP=1, FN=1, TP=0\n",
      "CC: Questioning the measurements and science: Methodologies/metrics used are unreliable/faulty => TN=38, FP=0, FN=2, TP=1\n",
      "CC: Questioning the measurements and science: Scientific community is unreliable => TN=39, FP=1, FN=0, TP=1\n",
      "Other => TN=24, FP=6, FN=1, TP=10\n",
      "URW: Amplifying war-related fears: By continuing the war we risk WWIII => TN=40, FP=0, FN=1, TP=0\n",
      "URW: Amplifying war-related fears: There is a real possibility that nuclear weapons will be employed => TN=37, FP=2, FN=2, TP=0\n",
      "URW: Blaming the war on others rather than the invader: Other => TN=40, FP=1, FN=0, TP=0\n",
      "URW: Blaming the war on others rather than the invader: The West are the aggressors => TN=35, FP=0, FN=2, TP=4\n",
      "URW: Blaming the war on others rather than the invader: Ukraine is the aggressor => TN=40, FP=0, FN=1, TP=0\n",
      "URW: Discrediting Ukraine: Discrediting Ukrainian government and officials and policies => TN=37, FP=1, FN=0, TP=3\n",
      "URW: Discrediting Ukraine: Discrediting Ukrainian military => TN=38, FP=1, FN=1, TP=1\n",
      "URW: Discrediting Ukraine: Discrediting Ukrainian nation and society => TN=39, FP=1, FN=0, TP=1\n",
      "URW: Discrediting Ukraine: Other => TN=38, FP=2, FN=1, TP=0\n",
      "URW: Discrediting Ukraine: Situation in Ukraine is hopeless => TN=39, FP=1, FN=0, TP=1\n",
      "URW: Discrediting Ukraine: Ukraine is a hub for criminal activities => TN=40, FP=0, FN=0, TP=1\n",
      "URW: Discrediting Ukraine: Ukraine is a puppet of the West => TN=35, FP=3, FN=3, TP=0\n",
      "URW: Discrediting Ukraine: Ukraine is associated with nazism => TN=39, FP=0, FN=0, TP=2\n",
      "URW: Discrediting the West, Diplomacy: Diplomacy does/will not work => TN=38, FP=0, FN=2, TP=1\n",
      "URW: Discrediting the West, Diplomacy: Other => TN=34, FP=1, FN=4, TP=2\n",
      "URW: Discrediting the West, Diplomacy: The EU is divided => TN=40, FP=0, FN=0, TP=1\n",
      "URW: Discrediting the West, Diplomacy: The West does not care about Ukraine, only about its interests => TN=35, FP=2, FN=2, TP=2\n",
      "URW: Discrediting the West, Diplomacy: The West is overreacting => TN=39, FP=2, FN=0, TP=0\n",
      "URW: Discrediting the West, Diplomacy: The West is weak => TN=40, FP=0, FN=0, TP=1\n",
      "URW: Discrediting the West, Diplomacy: West is tired of Ukraine => TN=39, FP=2, FN=0, TP=0\n",
      "URW: Distrust towards Media: Western media is an instrument of propaganda => TN=37, FP=0, FN=2, TP=2\n",
      "URW: Hidden plots by secret schemes of powerful groups: Other => TN=40, FP=1, FN=0, TP=0\n",
      "URW: Negative Consequences for the West: Other => TN=39, FP=2, FN=0, TP=0\n",
      "URW: Negative Consequences for the West: Sanctions imposed by Western countries will backfire => TN=40, FP=0, FN=1, TP=0\n",
      "URW: Overpraising the West: The West belongs in the right side of history => TN=40, FP=0, FN=1, TP=0\n",
      "URW: Praise of Russia: Other => TN=40, FP=0, FN=1, TP=0\n",
      "URW: Praise of Russia: Praise of Russian President Vladimir Putin => TN=40, FP=0, FN=1, TP=0\n",
      "URW: Praise of Russia: Praise of Russian military might => TN=40, FP=0, FN=1, TP=0\n",
      "URW: Praise of Russia: Russia is a guarantor of peace and prosperity => TN=40, FP=0, FN=1, TP=0\n",
      "URW: Russia is the Victim: Other => TN=40, FP=0, FN=1, TP=0\n",
      "URW: Russia is the Victim: The West is russophobic => TN=40, FP=0, FN=1, TP=0\n",
      "URW: Speculating war outcomes: Other => TN=40, FP=1, FN=0, TP=0\n",
      "URW: Speculating war outcomes: Russian army is collapsing => TN=39, FP=0, FN=2, TP=0\n",
      "URW: Speculating war outcomes: Ukrainian army is collapsing => TN=38, FP=1, FN=2, TP=0\n"
     ]
    }
   ],
   "source": [
    "pred_file = \"devset/EN/subtask-2-annotations-4o-mini.txt\"\n",
    "gold_file = \"devset_gold_labels/EN/subtask-2-annotations.txt\"\n",
    "\n",
    "# 1) Load predicted and gold annotations\n",
    "pred_narr, pred_subnarr = load_annotation_dicts(pred_file)\n",
    "gold_narr, gold_subnarr = load_annotation_dicts(gold_file)\n",
    "\n",
    "# 2) Compute confusion matrices for narratives\n",
    "narr_conf_mats = compute_multilabel_confusion_matrices(pred_narr, gold_narr)\n",
    "\n",
    "# 3) Compute confusion matrices for subnarratives\n",
    "subnarr_conf_mats = compute_multilabel_confusion_matrices(pred_subnarr, gold_subnarr)\n",
    "\n",
    "# 4) Print or store results\n",
    "print(\"=== Confusion Matrices for Narratives ===\")\n",
    "for label, cm in narr_conf_mats.items():\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(f\"{label} => TN={tn}, FP={fp}, FN={fn}, TP={tp}\")\n",
    "\n",
    "print(\"\\n=== Confusion Matrices for Subnarratives ===\")\n",
    "for label, cm in subnarr_conf_mats.items():\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(f\"{label} => TN={tn}, FP={fp}, FN={fn}, TP={tp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def compute_classification_report(pred_dict, gold_dict):\n",
    "    # Collect all unique labels\n",
    "    all_labels = sorted({lab for labs in gold_dict.values() for lab in labs}.union(\n",
    "                        {lab for labs in pred_dict.values() for lab in labs}))\n",
    "\n",
    "    # Prepare a consistent order\n",
    "    label_to_index = {label: i for i, label in enumerate(all_labels)}\n",
    "\n",
    "    # Gather all file IDs\n",
    "    all_file_ids = set(pred_dict.keys()).union(gold_dict.keys())\n",
    "\n",
    "    # Build multi-hot vectors\n",
    "    y_true_multi = []\n",
    "    y_pred_multi = []\n",
    "    for file_id in all_file_ids:\n",
    "        gold_set = gold_dict.get(file_id, set())\n",
    "        pred_set = pred_dict.get(file_id, set())\n",
    "        \n",
    "        gold_vec = [1 if label in gold_set else 0 for label in all_labels]\n",
    "        pred_vec = [1 if label in pred_set else 0 for label in all_labels]\n",
    "        \n",
    "        y_true_multi.append(gold_vec)\n",
    "        y_pred_multi.append(pred_vec)\n",
    "    \n",
    "    # average=\"macro\" or \"micro\", depending on your preference\n",
    "    report = classification_report(y_true_multi, y_pred_multi, target_names=all_labels, zero_division=0)\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_global_confusion_matrix(pred_dict, gold_dict):\n",
    "    \"\"\"\n",
    "    Computes the sum of TP, FP, FN, TN across all labels.\n",
    "    Returns: (total_TP, total_FP, total_FN, total_TN)\n",
    "    \"\"\"\n",
    "    # Gather all unique labels across predictions & ground truth\n",
    "    all_labels = set()\n",
    "    for file_id in pred_dict:\n",
    "        all_labels.update(pred_dict[file_id])\n",
    "    for file_id in gold_dict:\n",
    "        all_labels.update(gold_dict[file_id])\n",
    "\n",
    "    # Initialize global confusion matrix counts\n",
    "    total_TP, total_FP, total_FN, total_TN = 0, 0, 0, 0\n",
    "\n",
    "    # Union of all file IDs\n",
    "    all_file_ids = set(pred_dict.keys()).union(gold_dict.keys())\n",
    "\n",
    "    for label in all_labels:\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        for file_id in all_file_ids:\n",
    "            gold_labels = gold_dict.get(file_id, set())\n",
    "            pred_labels = pred_dict.get(file_id, set())\n",
    "\n",
    "            y_true.append(1 if label in gold_labels else 0)\n",
    "            y_pred.append(1 if label in pred_labels else 0)\n",
    "\n",
    "        # Compute confusion matrix for this label\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "        # Accumulate values\n",
    "        total_TP += tp\n",
    "        total_FP += fp\n",
    "        total_FN += fn\n",
    "        total_TN += tn\n",
    "\n",
    "    return total_TP, total_FP, total_FN, total_TN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Global Confusion Matrix for Narratives ===\n",
      "TP: 43, FP: 39, FN: 42, TN: 778\n",
      "\n",
      "=== Global Confusion Matrix for Subnarratives ===\n",
      "TP: 52, FP: 62, FN: 62, TN: 2407\n",
      "\n",
      "=== Global Performance Metrics ===\n",
      "Narrative Precision: 0.5244, Recall: 0.5059, F1-score: 0.5150\n",
      "Subnarrative Precision: 0.4561, Recall: 0.4561, F1-score: 0.4561\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "pred_file = \"devset/EN/subtask-2-annotations.txt\"\n",
    "gold_file = \"devset_gold_labels/EN/subtask-2-annotations.txt\"\n",
    "\n",
    "# Load annotations\n",
    "pred_narr, pred_subnarr = load_annotation_dicts(pred_file)\n",
    "gold_narr, gold_subnarr = load_annotation_dicts(gold_file)\n",
    "\n",
    "# Compute global confusion matrices\n",
    "global_TP_narr, global_FP_narr, global_FN_narr, global_TN_narr = compute_global_confusion_matrix(pred_narr, gold_narr)\n",
    "global_TP_subnarr, global_FP_subnarr, global_FN_subnarr, global_TN_subnarr = compute_global_confusion_matrix(pred_subnarr, gold_subnarr)\n",
    "\n",
    "# Print results\n",
    "print(\"=== Global Confusion Matrix for Narratives ===\")\n",
    "print(f\"TP: {global_TP_narr}, FP: {global_FP_narr}, FN: {global_FN_narr}, TN: {global_TN_narr}\")\n",
    "\n",
    "print(\"\\n=== Global Confusion Matrix for Subnarratives ===\")\n",
    "print(f\"TP: {global_TP_subnarr}, FP: {global_FP_subnarr}, FN: {global_FN_subnarr}, TN: {global_TN_subnarr}\")\n",
    "\n",
    "# Global Performance Metrics\n",
    "precision_narr = global_TP_narr / (global_TP_narr + global_FP_narr) if (global_TP_narr + global_FP_narr) > 0 else 0\n",
    "recall_narr = global_TP_narr / (global_TP_narr + global_FN_narr) if (global_TP_narr + global_FN_narr) > 0 else 0\n",
    "f1_narr = 2 * (precision_narr * recall_narr) / (precision_narr + recall_narr) if (precision_narr + recall_narr) > 0 else 0\n",
    "\n",
    "precision_subnarr = global_TP_subnarr / (global_TP_subnarr + global_FP_subnarr) if (global_TP_subnarr + global_FP_subnarr) > 0 else 0\n",
    "recall_subnarr = global_TP_subnarr / (global_TP_subnarr + global_FN_subnarr) if (global_TP_subnarr + global_FN_subnarr) > 0 else 0\n",
    "f1_subnarr = 2 * (precision_subnarr * recall_subnarr) / (precision_subnarr + recall_subnarr) if (precision_subnarr + recall_subnarr) > 0 else 0\n",
    "\n",
    "print(\"\\n=== Global Performance Metrics ===\")\n",
    "print(f\"Narrative Precision: {precision_narr:.4f}, Recall: {recall_narr:.4f}, F1-score: {f1_narr:.4f}\")\n",
    "print(f\"Subnarrative Precision: {precision_subnarr:.4f}, Recall: {recall_subnarr:.4f}, F1-score: {f1_subnarr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_narrative_for_text(text):\n",
    "    chat_result = narratives_user_proxy_agent.initiate_chat(\n",
    "            narratives_manager,\n",
    "            message=\"Here is the text that needs to be classified: \\n```{}```\\n ### \\n You are ONLY allowed to reply with '0' or '1'\".format(text),\n",
    "            summary_method='reflection_with_llm'\n",
    "        )\n",
    "    recognized_narratives = extract_recognized_narratives(chat_result.chat_history)\n",
    "    return recognized_narratives\n",
    "\n",
    "def extract_subnarratives_for_text(narratives, text):\n",
    "    subnarratives = []\n",
    "    for narrative in narratives:\n",
    "        subnarratives.append(extract_subnarratives_for_one_narrative(narrative, text))\n",
    "    return subnarratives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser\u001b[0m (to chat_manager):\n",
      "\n",
      "Here is the text that needs to be classified: \n",
      "```The study, published in Environmental Research Letters, reveals significant changes in the relationship between vegetation growth and water availability in the Northern Hemisphere's mid-latitudes over the past three decades. The research, led by Yang Song and colleagues, highlights the impact of elevated carbon dioxide (CO2) levels on this relationship, suggesting a closer relationship between vegetation growth and water availability than previously understood. The very compound that the Democrats are targeting â€“ CO2 â€“ is actually the solution to preserving croplands, grasslands, forests and water supplies for growing populations.```\n",
      " ### \n",
      " You are ONLY allowed to reply with '0' or '1'\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Authentication Fails (no such user)', 'type': 'authentication_error', 'param': None, 'code': 'invalid_request_error'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe study, published in Environmental Research Letters, reveals significant changes in the relationship between vegetation growth and water availability in the Northern Hemisphere\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms mid-latitudes over the past three decades. The research, led by Yang Song and colleagues, highlights the impact of elevated carbon dioxide (CO2) levels on this relationship, suggesting a closer relationship between vegetation growth and water availability than previously understood. The very compound that the Democrats are targeting â€“ CO2 â€“ is actually the solution to preserving croplands, grasslands, forests and water supplies for growing populations.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m extracted_narratives \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[43mextract_narrative_for_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m      4\u001b[0m extracted_subnarratives \u001b[38;5;241m=\u001b[39m extract_subnarratives_for_text(extracted_narratives, text)\n",
      "Cell \u001b[0;32mIn[45], line 2\u001b[0m, in \u001b[0;36mextract_narrative_for_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_narrative_for_text\u001b[39m(text):\n\u001b[0;32m----> 2\u001b[0m     chat_result \u001b[38;5;241m=\u001b[39m \u001b[43mnarratives_user_proxy_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnarratives_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHere is the text that needs to be classified: \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m```\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m```\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m ### \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m You are ONLY allowed to reply with \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m or \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43msummary_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreflection_with_llm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     recognized_narratives \u001b[38;5;241m=\u001b[39m extract_recognized_narratives(chat_result\u001b[38;5;241m.\u001b[39mchat_history)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m recognized_narratives\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:1125\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1124\u001b[0m         msg2send \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_init_message(message, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1125\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg2send\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1126\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_summarize_chat(\n\u001b[1;32m   1127\u001b[0m     summary_method,\n\u001b[1;32m   1128\u001b[0m     summary_args,\n\u001b[1;32m   1129\u001b[0m     recipient,\n\u001b[1;32m   1130\u001b[0m     cache\u001b[38;5;241m=\u001b[39mcache,\n\u001b[1;32m   1131\u001b[0m )\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mself\u001b[39m, recipient]:\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:817\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    815\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient, is_sending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 817\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    821\u001b[0m     )\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:925\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    924\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 925\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(reply, sender, silent\u001b[38;5;241m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:2066\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, **kwargs)\u001b[0m\n\u001b[1;32m   2064\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   2065\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[0;32m-> 2066\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2067\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[1;32m   2068\u001b[0m         log_event(\n\u001b[1;32m   2069\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2070\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreply_func_executed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2074\u001b[0m             reply\u001b[38;5;241m=\u001b[39mreply,\n\u001b[1;32m   2075\u001b[0m         )\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/groupchat.py:1169\u001b[0m, in \u001b[0;36mGroupChatManager.run_chat\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;66;03m# select the next speaker\u001b[39;00m\n\u001b[0;32m-> 1169\u001b[0m     speaker \u001b[38;5;241m=\u001b[39m \u001b[43mgroupchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_speaker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspeaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m silent:\n\u001b[1;32m   1171\u001b[0m         iostream \u001b[38;5;241m=\u001b[39m IOStream\u001b[38;5;241m.\u001b[39mget_default()\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/groupchat.py:570\u001b[0m, in \u001b[0;36mGroupChat.select_speaker\u001b[0;34m(self, last_speaker, selector)\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_agent(last_speaker)\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# auto speaker selection with 2-agent chat\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_auto_select_speaker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_speaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/groupchat.py:746\u001b[0m, in \u001b[0;36mGroupChat._auto_select_speaker\u001b[0;34m(self, last_speaker, selector, messages, agents)\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_speaker_selection_transforms\u001b[38;5;241m.\u001b[39madd_to_agent(speaker_selection_agent)\n\u001b[1;32m    745\u001b[0m \u001b[38;5;66;03m# Run the speaker selection chat\u001b[39;00m\n\u001b[0;32m--> 746\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mchecking_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeaker_selection_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# don't use caching for the speaker selection chat\u001b[39;49;00m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_turns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_attempts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Limiting the chat to the number of attempts, including the initial one\u001b[39;49;00m\n\u001b[1;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclear_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_speaker_auto_verbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Base silence on the verbose attribute\u001b[39;49;00m\n\u001b[1;32m    754\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_speaker_selection_result(result, last_speaker, agents)\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:1118\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m msg2send \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1117\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1118\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg2send\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_chat(recipient, clear_history)\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:817\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    815\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient, is_sending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 817\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    821\u001b[0m     )\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:925\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    924\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 925\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(reply, sender, silent\u001b[38;5;241m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:2066\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, **kwargs)\u001b[0m\n\u001b[1;32m   2064\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   2065\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[0;32m-> 2066\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2067\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[1;32m   2068\u001b[0m         log_event(\n\u001b[1;32m   2069\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2070\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreply_func_executed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2074\u001b[0m             reply\u001b[38;5;241m=\u001b[39mreply,\n\u001b[1;32m   2075\u001b[0m         )\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:1443\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m messages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1442\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_oai_messages[sender]\n\u001b[0;32m-> 1443\u001b[0m extracted_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_oai_reply_from_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_oai_system_message\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient_cache\u001b[49m\n\u001b[1;32m   1445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, extracted_response)\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:1462\u001b[0m, in \u001b[0;36mConversableAgent._generate_oai_reply_from_client\u001b[0;34m(self, llm_client, messages, cache)\u001b[0m\n\u001b[1;32m   1459\u001b[0m         all_messages\u001b[38;5;241m.\u001b[39mappend(message)\n\u001b[1;32m   1461\u001b[0m \u001b[38;5;66;03m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[0;32m-> 1462\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1466\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1467\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1468\u001b[0m extracted_response \u001b[38;5;241m=\u001b[39m llm_client\u001b[38;5;241m.\u001b[39mextract_text_or_completion_object(response)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/oai/client.py:964\u001b[0m, in \u001b[0;36mOpenAIWrapper.create\u001b[0;34m(self, **config)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    963\u001b[0m     request_ts \u001b[38;5;241m=\u001b[39m get_current_ts()\n\u001b[0;32m--> 964\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m APITimeoutError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    966\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/oai/client.py:468\u001b[0m, in \u001b[0;36mOpenAIClient.create\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_reasoning_model_params(params)\n\u001b[1;32m    467\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 468\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_or_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# remove the system_message from the response and add it in the prompt at the start.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_o1:\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/oai/client.py:302\u001b[0m, in \u001b[0;36mOpenAIClient._handle_openai_bad_request_error.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mBadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    304\u001b[0m         response_json \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/openai/resources/chat/completions.py:850\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    810\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    847\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    848\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    849\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 850\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/openai/_base_client.py:1283\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1271\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1279\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1280\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1281\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1282\u001b[0m     )\n\u001b[0;32m-> 1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/openai/_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/openai/_base_client.py:1064\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1063\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1064\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1067\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1068\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1072\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1073\u001b[0m )\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Authentication Fails (no such user)', 'type': 'authentication_error', 'param': None, 'code': 'invalid_request_error'}}"
     ]
    }
   ],
   "source": [
    "text = \"The study, published in Environmental Research Letters, reveals significant changes in the relationship between vegetation growth and water availability in the Northern Hemisphere's mid-latitudes over the past three decades. The research, led by Yang Song and colleagues, highlights the impact of elevated carbon dioxide (CO2) levels on this relationship, suggesting a closer relationship between vegetation growth and water availability than previously understood. The very compound that the Democrats are targeting â€“ CO2 â€“ is actually the solution to preserving croplands, grasslands, forests and water supplies for growing populations.\"\n",
    "\n",
    "extracted_narratives = list(set(extract_narrative_for_text(text)))\n",
    "extracted_subnarratives = extract_subnarratives_for_text(extracted_narratives, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CC: Climate change is beneficial']\n",
      "[['CC: Climate change is beneficial: CO2 is beneficial']]\n"
     ]
    }
   ],
   "source": [
    "print(extracted_narratives)\n",
    "print(extracted_subnarratives)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
