{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to extract the list of all unique subnarratives\n",
    "def get_subnarratives_list(file):\n",
    "    \"\"\"\n",
    "    Extracts subnarratives from the nested JSON structure.\n",
    "    \n",
    "    Args:\n",
    "        data (dict): The JSON-like dictionary containing narratives.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of subnarratives with the hierarchy preserved in their names.\n",
    "    \"\"\"\n",
    "    subnarratives = [\"Other\"]\n",
    "    with open(file, 'r') as f:    \n",
    "        data = json.load(f)\n",
    "        for main_category, subcategories in data.items():\n",
    "            for subcategory, narratives in subcategories.items():\n",
    "                if \"Other\" not in narratives:\n",
    "                    narratives.append(\"Other\")\n",
    "\n",
    "                for narrative in narratives:\n",
    "                    subnarratives.append(f\"{main_category}: {subcategory}: {narrative}\")\n",
    "        \n",
    "    return subnarratives\n",
    "\n",
    "def get_narratives_list(file):\n",
    "    \"\"\"\n",
    "    Extracts narratives from the nested JSON structure.\n",
    "    \n",
    "    Args:\n",
    "        data (dict): The JSON-like dictionary containing narratives.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of narratives with the hierarchy preserved in their names.\n",
    "    \"\"\"\n",
    "    narratives = [\"Other\"]\n",
    "    with open(file, 'r') as f:    \n",
    "        data = json.load(f)\n",
    "        for main_category, subcategories in data.items():\n",
    "            for subcategory, narrative in subcategories.items():\n",
    "                narratives.append(f\"{main_category}: {subcategory}\")\n",
    "        \n",
    "    return narratives\n",
    "\n",
    "import os\n",
    "def read_text(file_id, base_path='data/EN/raw-documents'):\n",
    "    with open(os.path.join(base_path, f'{file_id}'), 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "    \n",
    "def get_sibling_subnarratives(subnarrative, taxonomy):\n",
    "    \"\"\"\n",
    "    Get all the sibling subnarratives of the given subnarrative.\n",
    "    \n",
    "    Args:\n",
    "        subnarrative (str): The subnarrative for which to find siblings.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of sibling subnarratives.\n",
    "    \"\"\"\n",
    "    return taxonomy[subnarrative.split(': ')[0]][subnarrative.split(': ')[1]]\n",
    "\n",
    "def get_narrative_definition(narrative, narratives_list):\n",
    "    if narrative == 'Other':\n",
    "        return 'Statements that are NOT related to anyone of these topics : {}'.format(', '.join([get_narrative_short_name(narrative) for narrative in narratives_list if narrative != 'Other']))\n",
    "    narrative_definitions = pd.read_csv('data/narratives definition.csv')\n",
    "    short_name = narrative.split(':')[-1].strip()\n",
    "    return narrative_definitions[narrative_definitions['narrative'] == short_name]['definition'].values[0]\n",
    "\n",
    "def get_narrative_examples(narrative):\n",
    "    if narrative == 'Other':\n",
    "        return None\n",
    "    narrative_definitions = pd.read_csv('data/narratives definition.csv')\n",
    "    short_name = narrative.split(':')[-1].strip()\n",
    "    return narrative_definitions[narrative_definitions['narrative'] == short_name]['example'].values[0]\n",
    "\n",
    "def get_narrative_short_name(narrative):\n",
    "    return narrative.split(':')[-1].strip()\n",
    "\n",
    "def get_subnarrative_definition(subnarrative, narratives_list, taxonomy):\n",
    "    if subnarrative == 'Other':\n",
    "        return 'Statements that are NOT related to anyone of these narratives : {}'.format(', '.join([narrative for narrative in narratives_list if narrative != 'Other']))\n",
    "    short_name = subnarrative.split(':')[-1].strip()\n",
    "    narrative = subnarrative.split(':')[-2].strip()\n",
    "    narrative_defintion = get_narrative_definition(narrative, narratives_list)\n",
    "    if short_name == 'Other':\n",
    "        return 'Statement that are related to the narrative \"{}\", defined as {} but are not related to anyone of these subnarratives : {}'.format(get_narrative_short_name(narrative), narrative_defintion ,get_sibling_subnarratives(subnarrative, taxonomy))\n",
    "    subnarrative_definitions = pd.read_csv('data/subnarrative definitions.csv')\n",
    "    return subnarrative_definitions[subnarrative_definitions['subnarrative'] == short_name]['definition'].values[0]\n",
    "\n",
    "\n",
    "def get_subnarrative_examples(subnarrative):\n",
    "    if subnarrative == 'Other':\n",
    "        return None\n",
    "    short_name = subnarrative.split(':')[-1].strip()\n",
    "    if short_name == 'Other':\n",
    "        return None\n",
    "    subnarrative_definitions = pd.read_csv('data/subnarrative definitions.csv')\n",
    "    return subnarrative_definitions[subnarrative_definitions['subnarrative'] == short_name]['examples'].values[0]\n",
    "\n",
    "def get_subnarrative_short_name(subnarrative):\n",
    "    return subnarrative.split(':')[-1].strip()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_narratives_with_definitions_and_examples(taxonomy_file='data/taxonomy.json'):\n",
    "    \"\"\"\n",
    "    Builds a list of narratives from the taxonomy with their definitions and examples.\n",
    "    \n",
    "    Args:\n",
    "        taxonomy_file (str): Path to the JSON taxonomy file\n",
    "        \n",
    "    Returns:\n",
    "        list: List of dictionaries with narrative, definition, and examples\n",
    "    \"\"\"\n",
    "    narratives_data = {}\n",
    "    \n",
    "    # Get the list of narratives\n",
    "    narratives_list = get_narratives_list(taxonomy_file)\n",
    "    \n",
    "    # Create an entry for each narrative with its definition and examples\n",
    "    for narrative in narratives_list:\n",
    "        narratives_data[narrative] = {\n",
    "            \"definition\": get_narrative_definition(narrative, narratives_list),\n",
    "            \"examples\": get_narrative_examples(narrative)\n",
    "        }\n",
    "    \n",
    "    return narratives_data\n",
    "\n",
    "def build_subnarratives_with_definitions_and_examples(taxonomy_file='data/taxonomy.json'):\n",
    "    \"\"\"\n",
    "    Builds a list of subnarratives from the taxonomy with their definitions and examples.\n",
    "    \n",
    "    Args:\n",
    "        taxonomy_file (str): Path to the JSON taxonomy file\n",
    "        \n",
    "    Returns:\n",
    "        list: List of dictionaries with subnarrative, definition, and examples\n",
    "    \"\"\"\n",
    "    subnarratives_data = {}\n",
    "    \n",
    "    # Get the list of narratives and subnarratives\n",
    "    narratives_list = get_narratives_list(taxonomy_file)\n",
    "    subnarratives_list = get_subnarratives_list(taxonomy_file)\n",
    "    \n",
    "    # Load taxonomy data to use for subnarrative context\n",
    "    with open(taxonomy_file, 'r') as f:    \n",
    "        taxonomy = json.load(f)\n",
    "    \n",
    "    # Create an entry for each subnarrative with its definition and examples\n",
    "    for subnarrative in subnarratives_list:\n",
    "        subnarratives_data[subnarrative] = {\n",
    "            \"definition\": get_subnarrative_definition(subnarrative, narratives_list, taxonomy),\n",
    "            \"examples\": get_subnarrative_examples(subnarrative)\n",
    "        }\n",
    "    \n",
    "    return subnarratives_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "narratives = build_narratives_with_definitions_and_examples()\n",
    "subnarratives = build_subnarratives_with_definitions_and_examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "narrative_system_prompt = (\n",
    "    \"<instruction>\"\n",
    "    \"You are a highly precise binary classification model trained to determine whether a given text explicitly relates to the narrative: '{}'. \"\n",
    "    \"This narrative is defined as follows: ```{}```. \"\n",
    "    \"Here are clear and representative examples of statements that are related to this narrative: ```{}```. \"\n",
    "    \"Your task is to classify the given text strictly based on whether it contains explicit, unambiguous references to this narrative. \"\n",
    "    \"If the text directly aligns with the narrative's definition and examples, you MUST respond with '1'. \"\n",
    "    \"If there is any uncertainty, indirect reference, or ambiguity, you MUST respond with '0'. \"\n",
    "    \"Output only with a single character. Your answer MUST be strictly '1' or '0'—no explanations, no justifications, and no additional text. \"\n",
    "    \"If the narrative is only weakly implied or suggested, you MUST classify it as '0'. \"\n",
    "    \"</instruction>\"\n",
    ")\n",
    "\n",
    "narrative_user_prompt = (\n",
    "    \"Please classify the following text as related to the narrative '{}' or not. \"\n",
    "    \"Text\\n\"\n",
    "    \"```{}```\"\n",
    ")\n",
    "\n",
    "subnarrative_system_prompt = (\n",
    "    \"<instruction>\"\n",
    "    \"You are a highly precise binary classification model trained to determine whether a given text explicitly relates to the subnarrative: '{}'. \"\n",
    "    \"This subnarrative is defined as follows: ```{}```. \"\n",
    "    \"Here are clear and representative examples of statements that are related to this subnarrative: ```{}```. \"\n",
    "    \"Your task is to classify the given text strictly based on whether it contains explicit, unambiguous references to this subnarrative. \"\n",
    "    \"Output only with a single character. If the text directly aligns with the subnarrative's definition and examples, respond with '1'. Otherwise, respond with '0'. \"\n",
    "    \"You MUST classify as '0' if the connection is weak, indirect, implicit, or uncertain. \"\n",
    "    \"You are STRICTLY FORBIDDEN from providing any explanation, justification, or additional text. \"\n",
    "    \"ONLY respond with '1' or '0' based on clear and explicit evidence in the text.\"\n",
    "    \"</instruction>\"\n",
    ")\n",
    "\n",
    "subnarrative_user_prompt = (\n",
    "    \"Please classify the following text as related to the subnarrative '{}' or not. \"\n",
    "    \"Text\\n\"\n",
    "    \"```{}```\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating group chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "def create_narrative_agents(narratives_dict):\n",
    "    import hashlib\n",
    "    narrative_agents = {}\n",
    "    narratives_list = get_narratives_list('data/taxonomy.json')\n",
    "    llm_config={\n",
    "                \"config_list\": [\n",
    "                    {\n",
    "                        \"model\": \"gpt-4o\",\n",
    "                        \"api_key\": os.environ.get(\"OPENAI_API_KEY\")\n",
    "                    }\n",
    "                ],\n",
    "                'temperature': 0\n",
    "            }\n",
    "    for narrative, data in narratives_dict.items():\n",
    "        short_hash = hashlib.md5(narrative.encode()).hexdigest()[:6]\n",
    "        key = f\"agent_{short_hash}\"\n",
    "        \n",
    "        agent = autogen.AssistantAgent(\n",
    "            name=key,\n",
    "            system_message=narrative_system_prompt.format(narrative, data['definition'], data['examples']),\n",
    "            llm_config=llm_config\n",
    "        )\n",
    "\n",
    "        agent.description = (\n",
    "            \"I am a classification model trained to do classify whether a given text is related to the following narrative: {}. \"\n",
    "            \"I will be looking for {}\"\n",
    "        ).format(get_narrative_short_name(narrative), get_narrative_definition(narrative, narratives_list))\n",
    "\n",
    "        narrative_agents[key] = {\"agent\": agent, \"narrative\": narrative}\n",
    "\n",
    "    return narrative_agents\n",
    "    \n",
    "def create_narratives_group_chat(narrative_agents):\n",
    "    narratives_user_proxy_agent = autogen.UserProxyAgent(\n",
    "        name=\"user\",\n",
    "        code_execution_config=False,\n",
    "        llm_config={\n",
    "            \"config_list\": [\n",
    "                {\n",
    "                    \"model\": \"gpt-4o-mini\",\n",
    "                    \"api_key\": os.environ.get(\"OPENAI_API_KEY\")\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        human_input_mode='NEVER'\n",
    "    )\n",
    "\n",
    "    # Extract just the agent objects from the dictionary values\n",
    "    narrative_agent_objects = [agent_data[\"agent\"] for agent_data in narrative_agents.values()]\n",
    "\n",
    "    allowed_transitions = {}\n",
    "\n",
    "    for agent_key in narrative_agents:\n",
    "        allowed_transitions[narrative_agents[agent_key][\"agent\"]] = [narratives_user_proxy_agent]\n",
    "\n",
    "    narratives_group_chat = autogen.GroupChat(\n",
    "        agents= [narratives_user_proxy_agent] + narrative_agent_objects,\n",
    "        messages=[],\n",
    "        max_round=6,\n",
    "        send_introductions=True,\n",
    "        allowed_or_disallowed_speaker_transitions=allowed_transitions,\n",
    "        speaker_transitions_type=\"disallowed\",\n",
    "    )\n",
    "\n",
    "    narratives_manager = autogen.GroupChatManager(\n",
    "        groupchat=narratives_group_chat,\n",
    "        llm_config = {\n",
    "            \"config_list\": [\n",
    "                {\n",
    "                    \"model\": \"gpt-4o\",\n",
    "                    \"api_key\": os.environ.get(\"OPENAI_API_KEY\")\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return narratives_group_chat, narratives_manager, narratives_user_proxy_agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subnarratives_agents(subnarratives_dict):\n",
    "    import hashlib\n",
    "    subnarrative_agents = {}\n",
    "    narratives_list = get_narratives_list('data/taxonomy.json')\n",
    "    with open('data/taxonomy.json', 'r') as f:    \n",
    "        taxonomy = json.load(f)\n",
    "    llm_config={\n",
    "                \"config_list\": [\n",
    "                    {\n",
    "                        \"model\": \"gpt-4o\",\n",
    "                        \"api_key\": os.environ.get(\"OPENAI_API_KEY\")\n",
    "                    }\n",
    "                ],\n",
    "                'temperature': 0\n",
    "            }\n",
    "    for subnarrative, data in subnarratives_dict.items():\n",
    "        short_hash = hashlib.md5(subnarrative.encode()).hexdigest()[:6]\n",
    "        key = f\"agent_{short_hash}\"\n",
    "        agent = autogen.AssistantAgent(\n",
    "            name=key,\n",
    "            system_message=subnarrative_system_prompt.format(subnarrative, data['definition'], data['examples']),\n",
    "            llm_config=llm_config\n",
    "        )\n",
    "\n",
    "        agent.description = (\n",
    "            \"I am a classification model trained to do classify whether a given text is related to the following subnarrative: {}. \"\n",
    "            \"I will be looking for {}\"\n",
    "        ).format(get_subnarrative_short_name(subnarrative), get_subnarrative_definition(subnarrative, narratives_list, taxonomy))\n",
    "\n",
    "        subnarrative_agents[key] = {\"agent\": agent, \"subnarrative\": subnarrative}\n",
    "\n",
    "    return subnarrative_agents\n",
    "\n",
    "def create_subnarratives_group_chat(subnarrative_agents):\n",
    "    subnarratives_user_proxy_agent = autogen.UserProxyAgent(\n",
    "        name=\"user\",\n",
    "        code_execution_config=False,\n",
    "        llm_config={\n",
    "            \"config_list\": [\n",
    "                {\n",
    "                    \"model\": \"gpt-4o-mini\",\n",
    "                    \"api_key\": os.environ.get(\"OPENAI_API_KEY\")\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        human_input_mode='NEVER'\n",
    "    )\n",
    "    # Extract just the agent objects from the dictionary values\n",
    "    subnarrative_agent_objects = [agent_data[\"agent\"] for agent_data in subnarrative_agents.values()]\n",
    "\n",
    "    allowed_transitions = {}\n",
    "\n",
    "    for agent_key in subnarrative_agents:\n",
    "        allowed_transitions[subnarrative_agents[agent_key][\"agent\"]] = [subnarratives_user_proxy_agent]\n",
    "\n",
    "    subnarrative_group_chat = autogen.GroupChat(\n",
    "        agents= [subnarratives_user_proxy_agent] + subnarrative_agent_objects,\n",
    "        messages=[],\n",
    "        max_round=len(subnarrative_agents) + 1,\n",
    "        send_introductions=True,\n",
    "        allowed_or_disallowed_speaker_transitions=allowed_transitions,\n",
    "        speaker_transitions_type='disallowed',\n",
    "    )\n",
    "\n",
    "    subnarrative_manager = autogen.GroupChatManager(\n",
    "        groupchat=subnarrative_group_chat,\n",
    "        llm_config = {\n",
    "            \"config_list\": [\n",
    "                {\n",
    "                    \"model\": \"gpt-4o\",\n",
    "                    \"api_key\": os.environ.get(\"OPENAI_API_KEY\")\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        system_message=\"You are a group chat manager. You are asked to give the classification task to the agents that look relevant to the topic\"\n",
    "    )\n",
    "    \n",
    "    return subnarrative_group_chat, subnarrative_manager, subnarratives_user_proxy_agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_subnarrative_agents_by_narrative(subnarrative_agents):\n",
    "    \"\"\"\n",
    "    Groups subnarrative agents by their corresponding narrative.\n",
    "    \n",
    "    Args:\n",
    "        subnarrative_agents (list): List of subnarrative agents.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary where keys are narratives and values are lists of corresponding subnarrative agents.\n",
    "    \"\"\"\n",
    "    grouped_agents = {}\n",
    "    \n",
    "\n",
    "    for agent_data in subnarrative_agents.values():\n",
    "        subnarrative = agent_data[\"subnarrative\"]\n",
    "        if subnarrative == 'Other':\n",
    "            continue\n",
    "        narrative = subnarrative.split(': ')[0] + ': ' + subnarrative.split(': ')[1]\n",
    "        if narrative not in grouped_agents:\n",
    "            grouped_agents[narrative] = []\n",
    "        grouped_agents[narrative].append(agent_data)\n",
    "    \n",
    "    return grouped_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_groupchat_for_narrative(narrative, grouped_agents):\n",
    "    return create_subnarratives_group_chat(grouped_agents[narrative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_agents(agent_list):\n",
    "    for agent in agent_list:\n",
    "        agent.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Narrative extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_recognized_narratives(chat_history):\n",
    "    recognized_narratives = []\n",
    "    for message in chat_history:\n",
    "        if message['name'] == 'user' or message['name'] == 'chat_manager':\n",
    "            continue\n",
    "        narrative = message['name'].replace('_', ' ').replace('-', ': ')\n",
    "        if message['content'] == '1':\n",
    "            recognized_narratives.append(narrative)\n",
    "    if len(recognized_narratives) == 0:\n",
    "        recognized_narratives.append('Other')\n",
    "\n",
    "    if len(recognized_narratives) > 1 and 'Other' in recognized_narratives:\n",
    "        recognized_narratives.remove('Other')\n",
    "    return recognized_narratives\n",
    "\n",
    "def extract_recognized_subnarratives(chat_history):\n",
    "    recognized_subnarratives = []\n",
    "    for message in chat_history:\n",
    "        if message['name'] == 'user' or message['name'] == 'chat_manager':\n",
    "            continue\n",
    "        subnarrative = message['name'].replace('_', ' ').replace('-', ': ')\n",
    "        if message['content'] == '1':\n",
    "            recognized_subnarratives.append(subnarrative)\n",
    "    if len(recognized_subnarratives) == 0:\n",
    "        recognized_subnarratives.append('Other')\n",
    "\n",
    "    if len(recognized_subnarratives) > 1 and 'Other' in recognized_subnarratives:\n",
    "        recognized_subnarratives.remove('Other')\n",
    "    return recognized_subnarratives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subnarratives_for_one_narrative(narrative, text, grouped_agents):\n",
    "    _ , manager, user_proxy_agent = create_groupchat_for_narrative(narrative, grouped_agents)\n",
    "    # We get the subnarratives that belong to the narrative\n",
    "    chat_result = user_proxy_agent.initiate_chat(\n",
    "        manager,\n",
    "        message=\"Here is the text that needs to be classified: \\n```{}```\\nYou are ONLY allowed to reply with '0' or '1'\".format(text),\n",
    "        summary_method='reflection_with_llm'\n",
    "    )\n",
    "    return extract_recognized_subnarratives(chat_result.chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "narrative_agents = create_narrative_agents(narratives)\n",
    "narratives_group_chat, narratives_manager, narratives_user_proxy_agent = create_narratives_group_chat(narrative_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "subnarrative_agents = create_subnarratives_agents(subnarratives)\n",
    "subnarrative_group_chat, subnarrative_manager, subnarratives_user_proxy_agent = create_subnarratives_group_chat(subnarrative_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_agents = group_subnarrative_agents_by_narrative(subnarrative_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text_with_narratives(text, narrative_agents, narratives_manager, narratives_user_proxy_agent):\n",
    "    \"\"\"\n",
    "    Classifies the text using narrative agents.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to classify.\n",
    "        narratives_group_chat (GroupChat): The group chat for narrative agents.\n",
    "        narratives_manager (GroupChatManager): The manager for the group chat.\n",
    "        narratives_user_proxy_agent (UserProxyAgent): The user proxy agent for the group chat.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of recognized narratives.\n",
    "    \"\"\"\n",
    "    chat_result = narratives_user_proxy_agent.initiate_chat(\n",
    "        narratives_manager,\n",
    "        message=\"Here is the text that needs to be classified: \\n```{}```\\n ### \\n You are ONLY allowed to reply with '0' or '1'\".format(text),\n",
    "        summary_method='reflection_with_llm'\n",
    "    ) \n",
    "\n",
    "    history = chat_result.chat_history\n",
    "    narratives = extract_recognized_narratives(history)\n",
    "    reset_agents(narrative_agents)\n",
    "    narratives_group_chat.reset()\n",
    "    narratives_user_proxy_agent.reset()\n",
    "\n",
    "    return narratives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(file_id, base_path='data/EN/raw-documents'):\n",
    "    with open(os.path.join(base_path, f'{file_id}'), 'r', encoding='utf-8') as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_text('CC_TEST_00000.txt', base_path='testset/EN/subtask-2-documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser\u001b[0m (to chat_manager):\n",
      "\n",
      "Here is the text that needs to be classified: \n",
      "```‘Absolute Genius’: How Three Alarmist Billionaires Bankrolled The Fake Climate Catastrophe \n",
      "\n",
      " Do you think that the constant catastrophizing of weather and climate in the mainstream media, politics, and science has just appeared by accident? [emphasis, links added]\n",
      "\n",
      "Over the last few years, the BBC and the Guardian, [working] as of one mind, decided to float improbable ‘tipping point’ scares under cover of ‘scientists say’, while UN officials concluded that we had two years to save a ‘boiling’ planet and the ubiquitous ‘Jim’ Dale has been given free rein to make it up as he goes along on Talk TV and GB News.\n",
      "\n",
      "Of course, all this didn’t suddenly happen.\n",
      "\n",
      "Each of these examples is a testament to an extraordinary corruption of the true scientific process – an “amazing tale” according to political science writer Roger Pielke Jr., “a story of how wealth and power sought to shape climate science in pursuit of political goals.”\n",
      "\n",
      "The main culprit in this amazing tale will not be unknown to regular readers of the Daily Sceptic and it is the improbable scenario of RCP8.5.\n",
      "\n",
      "This has been promoted as a ‘business as usual’ set of scientific, economic, and societal assumptions and it suggests a temperature rise of up to 4°C in less than 80 years.\n",
      "\n",
      "Although downgraded by the UN’s Intergovernmental Panel on Climate Change of late as “low likelihood”, it is still estimated to be behind about 50% of the climate-modeled ‘impacts’ highlighted over the entire scientific literature.\n",
      "\n",
      "Almost all the claims of climate Armageddon and economic disaster peddled by those identified in our first paragraph are based on RCP8.5 input.\n",
      "\n",
      "It is a tale of how three wealthy men bankrolled a project to promote an extreme scenario to guarantee that the economic impact of climate change they projected into the future would be “eye-poppingly large.”\n",
      "\n",
      "It will also not be a surprise to learn that the billionaire green activist Michael Bloomberg has played a key role over the last decade in lifting this implausible pathway to underserved prominence.\n",
      "\n",
      "Notes Pielke: “It’s a story of privilege and conceit – the privilege in American democracy that accompanies being mindbogglingly wealthy, and the conceit that climate policies can best be pursued by corrupting the scientific literature on climate change.”\n",
      "\n",
      "In 2012, three wealthy men, Bloomberg, hedge fund manager Tom Steyer, and former CEO of Goldman Sachs Hank Paulson chipped in $500,000 each to fund a project “making the climate threat feel real, immediate and potentially devastating to the business world.”\n",
      "\n",
      "An early funded report was part-titled ‘Risky Business’ and it focused on RCP8.5 “as the pathway closest to a business-as-usual trajectory”. The pathway was said to be “closest to a future without concerted action to reduce future warming.”\n",
      "\n",
      "In Pielke’s view, the authors of the ‘Risky Business’ report made two significant methodological mistakes. They characterized the extreme RCP8.5 scenario as ‘business as usual’ and suggested the world could move from one scenario to another.\n",
      "\n",
      "As Pielke notes, the four different scenarios are independent with, for instance, RCP2.5 assuming a world with three billion people fewer than RCP8.5.\n",
      "\n",
      "Both of the mistaken methodological choices were said to be contrary to the appropriate use set by the people who created them.\n",
      "\n",
      "The “genius” of ‘Risky Business’ was it undertook a “sophisticated campaign” to introduce its methodological ideas into the mainstream scientific literature, “where they would take on a life of their own”.\n",
      "\n",
      "In 2016, a paper from the ‘Risky Business’ project was published in the prestigious journal Science featuring the erroneous notion of moving from one RCP scenario to another.\n",
      "\n",
      "Despite the obvious methodological flaw, notes Pielke, the paper passed peer review with little or no criticism, and to date has been cited more than 1,100 times.\n",
      "\n",
      "“Hundreds, maybe thousands of papers followed similarly in adopting the same assumption of moving between incommensurate scenarios”, observes Pielke.\n",
      "\n",
      "A year later, Science published another study from ‘Risky Business’ with the bizarre suggestion that the United States would see a 10% hit to its economy under the most extreme version of RCP8.5, with an incredible 8°C rise in temperatures by the end of the century.\n",
      "\n",
      "This prominent paper has been cited more than 1,100 times in other studies and, noted Piekle, the 10% GDP loss would become the top-line conclusion of the U.S. National Climate Assessment the very next year.\n",
      "\n",
      "“Publishing papers in the academic literature based on the flawed ‘Risky Business’ methods was a formula that would be repeated time and time again. Like the introduction of a virus, the misleading reinterpretation of climate scenarios expanded throughout climate science,” states Pielke.\n",
      "\n",
      "The work begun by Bloomberg-Steyer-Paulson was subsequently taken up by a group called Climate Impact Lab, a collaboration of ‘Risky Business’ leaders and several universities.\n",
      "\n",
      "According to Pielke, Climate Impact Lab has thrived on using RCP8.5 to generate a steady series of media-friendly studies projecting extreme climate impacts.\n",
      "\n",
      "Among them are stories suggesting rising sea levels could swamp major cities and displace almost 200 million people, while a rise in climate-related deaths will surpass all infectious diseases.\n",
      "\n",
      "All backed, of course, by ‘scientists say’.\n",
      "\n",
      "Pielke concludes that there is no hidden conspiracy – “all of this is taking place in plain sight”.\n",
      "\n",
      "The political advocacy was “absolute genius” – a well-funded effort to fundamentally change how climate science was reported in the media, and ultimately how political discussion and policy options are shaped.\n",
      "\n",
      "“This effort has been phenomenally successful,” says Pielke – or as we might put it, the complete Dale-ification of most climate and net zero debate has been achieved.\n",
      "\n",
      "```\n",
      " ### \n",
      " You are ONLY allowed to reply with '0' or '1'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: CC-Hidden_plots_by_secret_schemes_of_powerful_groups\n",
      "\u001b[0m\n",
      "\u001b[33mCC-Hidden_plots_by_secret_schemes_of_powerful_groups\u001b[0m (to chat_manager):\n",
      "\n",
      "1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: CC-Criticism_of_institutions_and_authorities\n",
      "\u001b[0m\n",
      "\u001b[33mCC-Criticism_of_institutions_and_authorities\u001b[0m (to chat_manager):\n",
      "\n",
      "0\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: CC-Questioning_the_measurements_and_science\n",
      "\u001b[0m\n",
      "\u001b[33mCC-Questioning_the_measurements_and_science\u001b[0m (to chat_manager):\n",
      "\n",
      "0\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: CC-Criticism_of_institutions_and_authorities\n",
      "\u001b[0m\n",
      "\u001b[33mCC-Criticism_of_institutions_and_authorities\u001b[0m (to chat_manager):\n",
      "\n",
      "0\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: CC-Hidden_plots_by_secret_schemes_of_powerful_groups\n",
      "\u001b[0m\n",
      "\u001b[33mCC-Hidden_plots_by_secret_schemes_of_powerful_groups\u001b[0m (to chat_manager):\n",
      "\n",
      "0\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser\u001b[0m (to chat_manager):\n",
      "\n",
      "Here is the text that needs to be classified: \n",
      "```‘Absolute Genius’: How Three Alarmist Billionaires Bankrolled The Fake Climate Catastrophe \n",
      "\n",
      " Do you think that the constant catastrophizing of weather and climate in the mainstream media, politics, and science has just appeared by accident? [emphasis, links added]\n",
      "\n",
      "Over the last few years, the BBC and the Guardian, [working] as of one mind, decided to float improbable ‘tipping point’ scares under cover of ‘scientists say’, while UN officials concluded that we had two years to save a ‘boiling’ planet and the ubiquitous ‘Jim’ Dale has been given free rein to make it up as he goes along on Talk TV and GB News.\n",
      "\n",
      "Of course, all this didn’t suddenly happen.\n",
      "\n",
      "Each of these examples is a testament to an extraordinary corruption of the true scientific process – an “amazing tale” according to political science writer Roger Pielke Jr., “a story of how wealth and power sought to shape climate science in pursuit of political goals.”\n",
      "\n",
      "The main culprit in this amazing tale will not be unknown to regular readers of the Daily Sceptic and it is the improbable scenario of RCP8.5.\n",
      "\n",
      "This has been promoted as a ‘business as usual’ set of scientific, economic, and societal assumptions and it suggests a temperature rise of up to 4°C in less than 80 years.\n",
      "\n",
      "Although downgraded by the UN’s Intergovernmental Panel on Climate Change of late as “low likelihood”, it is still estimated to be behind about 50% of the climate-modeled ‘impacts’ highlighted over the entire scientific literature.\n",
      "\n",
      "Almost all the claims of climate Armageddon and economic disaster peddled by those identified in our first paragraph are based on RCP8.5 input.\n",
      "\n",
      "It is a tale of how three wealthy men bankrolled a project to promote an extreme scenario to guarantee that the economic impact of climate change they projected into the future would be “eye-poppingly large.”\n",
      "\n",
      "It will also not be a surprise to learn that the billionaire green activist Michael Bloomberg has played a key role over the last decade in lifting this implausible pathway to underserved prominence.\n",
      "\n",
      "Notes Pielke: “It’s a story of privilege and conceit – the privilege in American democracy that accompanies being mindbogglingly wealthy, and the conceit that climate policies can best be pursued by corrupting the scientific literature on climate change.”\n",
      "\n",
      "In 2012, three wealthy men, Bloomberg, hedge fund manager Tom Steyer, and former CEO of Goldman Sachs Hank Paulson chipped in $500,000 each to fund a project “making the climate threat feel real, immediate and potentially devastating to the business world.”\n",
      "\n",
      "An early funded report was part-titled ‘Risky Business’ and it focused on RCP8.5 “as the pathway closest to a business-as-usual trajectory”. The pathway was said to be “closest to a future without concerted action to reduce future warming.”\n",
      "\n",
      "In Pielke’s view, the authors of the ‘Risky Business’ report made two significant methodological mistakes. They characterized the extreme RCP8.5 scenario as ‘business as usual’ and suggested the world could move from one scenario to another.\n",
      "\n",
      "As Pielke notes, the four different scenarios are independent with, for instance, RCP2.5 assuming a world with three billion people fewer than RCP8.5.\n",
      "\n",
      "Both of the mistaken methodological choices were said to be contrary to the appropriate use set by the people who created them.\n",
      "\n",
      "The “genius” of ‘Risky Business’ was it undertook a “sophisticated campaign” to introduce its methodological ideas into the mainstream scientific literature, “where they would take on a life of their own”.\n",
      "\n",
      "In 2016, a paper from the ‘Risky Business’ project was published in the prestigious journal Science featuring the erroneous notion of moving from one RCP scenario to another.\n",
      "\n",
      "Despite the obvious methodological flaw, notes Pielke, the paper passed peer review with little or no criticism, and to date has been cited more than 1,100 times.\n",
      "\n",
      "“Hundreds, maybe thousands of papers followed similarly in adopting the same assumption of moving between incommensurate scenarios”, observes Pielke.\n",
      "\n",
      "A year later, Science published another study from ‘Risky Business’ with the bizarre suggestion that the United States would see a 10% hit to its economy under the most extreme version of RCP8.5, with an incredible 8°C rise in temperatures by the end of the century.\n",
      "\n",
      "This prominent paper has been cited more than 1,100 times in other studies and, noted Piekle, the 10% GDP loss would become the top-line conclusion of the U.S. National Climate Assessment the very next year.\n",
      "\n",
      "“Publishing papers in the academic literature based on the flawed ‘Risky Business’ methods was a formula that would be repeated time and time again. Like the introduction of a virus, the misleading reinterpretation of climate scenarios expanded throughout climate science,” states Pielke.\n",
      "\n",
      "The work begun by Bloomberg-Steyer-Paulson was subsequently taken up by a group called Climate Impact Lab, a collaboration of ‘Risky Business’ leaders and several universities.\n",
      "\n",
      "According to Pielke, Climate Impact Lab has thrived on using RCP8.5 to generate a steady series of media-friendly studies projecting extreme climate impacts.\n",
      "\n",
      "Among them are stories suggesting rising sea levels could swamp major cities and displace almost 200 million people, while a rise in climate-related deaths will surpass all infectious diseases.\n",
      "\n",
      "All backed, of course, by ‘scientists say’.\n",
      "\n",
      "Pielke concludes that there is no hidden conspiracy – “all of this is taking place in plain sight”.\n",
      "\n",
      "The political advocacy was “absolute genius” – a well-funded effort to fundamentally change how climate science was reported in the media, and ultimately how political discussion and policy options are shaped.\n",
      "\n",
      "“This effort has been phenomenally successful,” says Pielke – or as we might put it, the complete Dale-ification of most climate and net zero debate has been achieved.\n",
      "\n",
      "```\n",
      "You are ONLY allowed to reply with '0' or '1'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: CC-Hidden_plots_by_secret_schemes_of_powerful_groups-Blaming_global_elites\n",
      "\u001b[0m\n",
      "\u001b[33mCC-Hidden_plots_by_secret_schemes_of_powerful_groups-Blaming_global_elites\u001b[0m (to chat_manager):\n",
      "\n",
      "1\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Invalid 'messages[2].name': string too long. Expected a string with maximum length 64, but got a string with length 74 instead.\", 'type': 'invalid_request_error', 'param': 'messages[2].name', 'code': 'string_above_max_length'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m narratives \u001b[38;5;241m=\u001b[39m classify_text_with_narratives(text, narrative_agents, narratives_manager, narratives_user_proxy_agent)\n\u001b[1;32m      2\u001b[0m narrative \u001b[38;5;241m=\u001b[39m narratives[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m subnarratives \u001b[38;5;241m=\u001b[39m \u001b[43mextract_subnarratives_for_one_narrative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnarrative\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrouped_agents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNarrative:\u001b[39m\u001b[38;5;124m'\u001b[39m, narrative)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSubnarratives:\u001b[39m\u001b[38;5;124m'\u001b[39m, subnarratives)\n",
      "Cell \u001b[0;32mIn[46], line 4\u001b[0m, in \u001b[0;36mextract_subnarratives_for_one_narrative\u001b[0;34m(narrative, text, grouped_agents)\u001b[0m\n\u001b[1;32m      2\u001b[0m _ , manager, user_proxy_agent \u001b[38;5;241m=\u001b[39m create_groupchat_for_narrative(narrative, grouped_agents)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# We get the subnarratives that belong to the narrative\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m chat_result \u001b[38;5;241m=\u001b[39m \u001b[43muser_proxy_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHere is the text that needs to be classified: \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m```\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m```\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mYou are ONLY allowed to reply with \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m or \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msummary_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreflection_with_llm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extract_recognized_subnarratives(chat_result\u001b[38;5;241m.\u001b[39mchat_history)\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:1125\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1124\u001b[0m         msg2send \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_init_message(message, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1125\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg2send\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1126\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_summarize_chat(\n\u001b[1;32m   1127\u001b[0m     summary_method,\n\u001b[1;32m   1128\u001b[0m     summary_args,\n\u001b[1;32m   1129\u001b[0m     recipient,\n\u001b[1;32m   1130\u001b[0m     cache\u001b[38;5;241m=\u001b[39mcache,\n\u001b[1;32m   1131\u001b[0m )\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mself\u001b[39m, recipient]:\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:817\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    815\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient, is_sending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 817\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    821\u001b[0m     )\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:925\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    924\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 925\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(reply, sender, silent\u001b[38;5;241m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:2066\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, **kwargs)\u001b[0m\n\u001b[1;32m   2064\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   2065\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[0;32m-> 2066\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2067\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[1;32m   2068\u001b[0m         log_event(\n\u001b[1;32m   2069\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2070\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreply_func_executed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2074\u001b[0m             reply\u001b[38;5;241m=\u001b[39mreply,\n\u001b[1;32m   2075\u001b[0m         )\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/groupchat.py:1169\u001b[0m, in \u001b[0;36mGroupChatManager.run_chat\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;66;03m# select the next speaker\u001b[39;00m\n\u001b[0;32m-> 1169\u001b[0m     speaker \u001b[38;5;241m=\u001b[39m \u001b[43mgroupchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_speaker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspeaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m silent:\n\u001b[1;32m   1171\u001b[0m         iostream \u001b[38;5;241m=\u001b[39m IOStream\u001b[38;5;241m.\u001b[39mget_default()\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/groupchat.py:570\u001b[0m, in \u001b[0;36mGroupChat.select_speaker\u001b[0;34m(self, last_speaker, selector)\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_agent(last_speaker)\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# auto speaker selection with 2-agent chat\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_auto_select_speaker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_speaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/groupchat.py:746\u001b[0m, in \u001b[0;36mGroupChat._auto_select_speaker\u001b[0;34m(self, last_speaker, selector, messages, agents)\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_speaker_selection_transforms\u001b[38;5;241m.\u001b[39madd_to_agent(speaker_selection_agent)\n\u001b[1;32m    745\u001b[0m \u001b[38;5;66;03m# Run the speaker selection chat\u001b[39;00m\n\u001b[0;32m--> 746\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mchecking_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeaker_selection_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# don't use caching for the speaker selection chat\u001b[39;49;00m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_turns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_attempts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Limiting the chat to the number of attempts, including the initial one\u001b[39;49;00m\n\u001b[1;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclear_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_speaker_auto_verbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Base silence on the verbose attribute\u001b[39;49;00m\n\u001b[1;32m    754\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_speaker_selection_result(result, last_speaker, agents)\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:1118\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m msg2send \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1117\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1118\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg2send\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_chat(recipient, clear_history)\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:817\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    815\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient, is_sending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 817\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    821\u001b[0m     )\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:925\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    924\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 925\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(reply, sender, silent\u001b[38;5;241m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:2066\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, **kwargs)\u001b[0m\n\u001b[1;32m   2064\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   2065\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[0;32m-> 2066\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2067\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[1;32m   2068\u001b[0m         log_event(\n\u001b[1;32m   2069\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2070\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreply_func_executed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2074\u001b[0m             reply\u001b[38;5;241m=\u001b[39mreply,\n\u001b[1;32m   2075\u001b[0m         )\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:1443\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m messages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1442\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_oai_messages[sender]\n\u001b[0;32m-> 1443\u001b[0m extracted_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_oai_reply_from_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_oai_system_message\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient_cache\u001b[49m\n\u001b[1;32m   1445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, extracted_response)\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:1462\u001b[0m, in \u001b[0;36mConversableAgent._generate_oai_reply_from_client\u001b[0;34m(self, llm_client, messages, cache)\u001b[0m\n\u001b[1;32m   1459\u001b[0m         all_messages\u001b[38;5;241m.\u001b[39mappend(message)\n\u001b[1;32m   1461\u001b[0m \u001b[38;5;66;03m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[0;32m-> 1462\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1466\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1467\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1468\u001b[0m extracted_response \u001b[38;5;241m=\u001b[39m llm_client\u001b[38;5;241m.\u001b[39mextract_text_or_completion_object(response)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/oai/client.py:964\u001b[0m, in \u001b[0;36mOpenAIWrapper.create\u001b[0;34m(self, **config)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    963\u001b[0m     request_ts \u001b[38;5;241m=\u001b[39m get_current_ts()\n\u001b[0;32m--> 964\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m APITimeoutError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    966\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/oai/client.py:468\u001b[0m, in \u001b[0;36mOpenAIClient.create\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_reasoning_model_params(params)\n\u001b[1;32m    467\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 468\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_or_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# remove the system_message from the response and add it in the prompt at the start.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_o1:\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/oai/client.py:316\u001b[0m, in \u001b[0;36mOpenAIClient._handle_openai_bad_request_error.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m         error_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    309\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis error typically occurs when the agent name contains invalid characters, such as spaces or special symbols.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    310\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure that your agent name follows the correct format and doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt include any unsupported characters.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    311\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheck the agent name and try again.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    312\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHere is the full BadRequestError from openai:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m         )\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_message)\n\u001b[0;32m--> 316\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/autogen/oai/client.py:302\u001b[0m, in \u001b[0;36mOpenAIClient._handle_openai_bad_request_error.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mBadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    304\u001b[0m         response_json \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/openai/resources/chat/completions.py:850\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    810\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    847\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    848\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    849\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 850\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/openai/_base_client.py:1283\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1271\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1279\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1280\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1281\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1282\u001b[0m     )\n\u001b[0;32m-> 1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/openai/_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PHD-Track/.venv/lib/python3.12/site-packages/openai/_base_client.py:1064\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1063\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1064\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1067\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1068\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1072\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1073\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Invalid 'messages[2].name': string too long. Expected a string with maximum length 64, but got a string with length 74 instead.\", 'type': 'invalid_request_error', 'param': 'messages[2].name', 'code': 'string_above_max_length'}}"
     ]
    }
   ],
   "source": [
    "narratives = classify_text_with_narratives(text, narrative_agents, narratives_manager, narratives_user_proxy_agent)\n",
    "narrative = narratives[0]\n",
    "subnarratives = extract_subnarratives_for_one_narrative(narrative, text, grouped_agents)\n",
    "print('Narrative:', narrative)\n",
    "print('Subnarratives:', subnarratives)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
